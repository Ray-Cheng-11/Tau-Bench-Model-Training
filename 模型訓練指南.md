# GPT-OSS 20B Reinforcement Learning on Tau-Bench Tasks

Train GPT-OSS 20B model to perform customer service tool calling using Tau-Bench style tasks with Reinforcement Learning (GRPO) and Supervised Fine-Tuning (SFT).

[![Model](https://img.shields.io/badge/Model-GPT--OSS--20B-blue)](https://huggingface.co/unsloth/gpt-oss-20b)
[![Framework](https://img.shields.io/badge/Framework-Unsloth-green)](https://github.com/unslothai/unsloth)
[![Training](https://img.shields.io/badge/Training-GRPO-orange)](https://huggingface.co/docs/trl/grpo_trainer)

## üìã Overview

This project implements a two-phase training pipeline for fine-tuning GPT-OSS 20B on customer service tool calling tasks:

1. **Phase 1: SFT Pretraining** - Supervised fine-tuning on gold-standard tool calls
2. **Phase 2: GRPO Training** - Reinforcement learning for policy optimization

### Key Features

- ‚úÖ **Two-Phase Training**: SFT warm-up followed by RL refinement
- ‚úÖ **Memory Efficient**: Uses Unsloth for 70% VRAM reduction + 2-6x speedup
- ‚úÖ **Real Tool Execution**: Integrated with Tau-Bench retail environment
- ‚úÖ **Reward-Based Learning**: Tool-level cumulative rewards with action correctness
- ‚úÖ **Production Ready**: 4-bit quantization + LoRA for efficient deployment

## üöÄ Quick Start

### Prerequisites

```bash
# GPU Requirements
- NVIDIA GPU with 24GB+ VRAM (A100/A40 recommended)
- CUDA 11.8+

# Python Requirements
- Python 3.10+
- PyTorch 2.8.0+
- Transformers 4.56.2
```

### Training Environment Setup

This project supports training on multiple environments. Choose the one that fits your setup:

#### Option 1: Local Environment (Recommended for Development)

**Standard Setup**:
```bash
# Install Unsloth and dependencies
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
```

**For A100/H100 GPUs** (better performance):
```bash
pip install "unsloth[cu121-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

#### Option 2: NVIDIA DGX Spark (For Large-Scale Training)

The NVIDIA DGX Spark provides 128 GB of unified memory, allowing you to train models up to **200B parameters**. Perfect for training gpt-oss-120b or larger models.

**Features**:
- üí™ Train models up to 200B parameters
- üöÄ 128 GB unified memory
- ‚ö° Native Blackwell GPU support
- üê≥ Docker-based environment

**Setup Instructions**:

1. **Download the Dockerfile**:
```bash
sudo apt update && sudo apt install -y wget
wget -O Dockerfile "https://raw.githubusercontent.com/unslothai/notebooks/main/Dockerfile_DGX_Spark"
```

2. **Build the Docker Image**:
```bash
docker build -f Dockerfile -t unsloth-dgx-spark .
```

<details>
<summary><strong>Click to see the full DGX Spark Dockerfile</strong></summary>

```dockerfile
FROM nvcr.io/nvidia/pytorch:25.09-py3

# Set CUDA environment variables
ENV CUDA_HOME=/usr/local/cuda-13.0/
ENV CUDA_PATH=$CUDA_HOME
ENV PATH=$CUDA_HOME/bin:$PATH
ENV LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
ENV C_INCLUDE_PATH=$CUDA_HOME/include:$C_INCLUDE_PATH
ENV CPLUS_INCLUDE_PATH=$CUDA_HOME/include:$CPLUS_INCLUDE_PATH

# Install triton from source for latest blackwell support
RUN git clone https://github.com/triton-lang/triton.git && \
    cd triton && \
    git checkout c5d671f91d90f40900027382f98b17a3e04045f6 && \
    pip install -r python/requirements.txt && \
    pip install . && \
    cd ..

# Install xformers from source for blackwell support
RUN git clone --depth=1 https://github.com/facebookresearch/xformers --recursive && \
    cd xformers && \
    export TORCH_CUDA_ARCH_LIST="12.1" && \
    python setup.py install && \
    cd ..

# Install unsloth and other dependencies
RUN pip install unsloth unsloth_zoo bitsandbytes==0.48.0 transformers==4.56.2 trl==0.22.2

# Launch the shell
CMD ["/bin/bash"]
```
</details>

3. **Launch the Container**:
```bash
docker run -it \
    --gpus=all \
    --net=host \
    --ipc=host \
    --ulimit memlock=-1 \
    --ulimit stack=67108864 \
    -v $(pwd):$(pwd) \
    -v $HOME/.cache/huggingface:/root/.cache/huggingface \
    -w $(pwd) \
    unsloth-dgx-spark
```

4. **Inside the container, clone the project**:
```bash
git clone <your-repo-url>
cd APIGen-MT-5k
```

**Memory Usage**:
- gpt-oss-20b: ~24-40 GB VRAM (4-bit quantization)
- gpt-oss-120b: ~68 GB unified memory (QLoRA 4-bit)

**Resources**:
- üìö [Unsloth DGX Spark Documentation](https://unsloth.ai/docs/basics/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth)
- üíª [DGX Spark Dockerfile](https://raw.githubusercontent.com/unslothai/notebooks/main/Dockerfile_DGX_Spark)
- üìù [Example RL Notebook for DGX Spark](https://github.com/unslothai/notebooks/blob/main/nb/gpt_oss_(20B)_Reinforcement_Learning_2048_Game_DGX_Spark.ipynb)

#### Option 3: Cloud Platforms

**Google Colab** (Free tier with T4 GPU):
```python
# In Colab, run this at the start of your notebook
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
```

**AWS/Azure/GCP** (with A100/H100):
```bash
# Same as local setup
pip install "unsloth[cu121-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

**Kaggle**:
```python
# Kaggle supports P100/T4 GPUs
!pip install "unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git"
```

### Environment Verification

After installation, verify your environment:

```python
import torch
import unsloth

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"GPU count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"GPU name: {torch.cuda.get_device_name(0)}")
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
print(f"Unsloth version: {unsloth.__version__}")
```

**Expected output**:
```
PyTorch version: 2.8.0+cu121
CUDA available: True
CUDA version: 12.1
GPU count: 1
GPU name: NVIDIA A100-SXM4-80GB
GPU memory: 79.15 GB
Unsloth version: 2025.1.7
```

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd APIGen-MT-5k

# Install dependencies (automatically handled in notebook)
# The notebook will install:
# - Unsloth and dependencies
# - TRL (Transformer Reinforcement Learning)
# - Required tools for Tau-Bench
```

### Running the Notebook

1. **Open the notebook**:
   ```bash
   jupyter notebook gpt_oss_20b_tau_bench_rl_training.ipynb
   ```

2. **Execute cells sequentially**:
   - Cell 1-3: Model loading and setup (5-10 min)
   - Cell 4-15: Dataset preparation and reward function (2-5 min)
   - Cell 16: SFT pretraining (30-60 min)
   - Cell 17-21: GRPO training (2-4 hours)
   - Cell 22+: Evaluation (10-20 min)

3. **Monitor training**:
   ```bash
   # Optional: View TensorBoard logs
   tensorboard --logdir outputs/tau_bench_rl
   ```

### Quick Test

```python
# After training, test the model
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="outputs/tau_bench_rl/final_model"
)

# Test query
query = "Hi, I need to cancel my order #W1234567"
response = model.generate(query)
print(response)
```

## üìÅ Project Structure

```
APIGen-MT-5k/
‚îú‚îÄ‚îÄ gpt_oss_20b_tau_bench_rl_training.ipynb          # Main training notebook
‚îú‚îÄ‚îÄ dataset.jsonl                                     # Training dataset (800 tasks)
‚îú‚îÄ‚îÄ 3.5k.jsonl                                        # Extended dataset (3500+ tasks)
‚îÇ
‚îú‚îÄ‚îÄ configs.py                                        # Configuration settings
‚îú‚îÄ‚îÄ task_tester.py                                    # Task execution and testing
‚îú‚îÄ‚îÄ data_reader.py                                    # Dataset loading utilities
‚îú‚îÄ‚îÄ task_generator.py                                 # Task generation tools
‚îÇ
‚îú‚îÄ‚îÄ envs/retail/                                      # Retail environment
‚îÇ   ‚îú‚îÄ‚îÄ data/                                         # User/order/product data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ users.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orders.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ products.json
‚îÇ   ‚îú‚îÄ‚îÄ tools/                                        # Tool implementations
‚îÇ   ‚îî‚îÄ‚îÄ wiki.md                                       # Domain policies
‚îÇ
‚îú‚îÄ‚îÄ outputs/                                          # Training outputs (generated)
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-*/                                 # Training checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ final_model/                                  # Final trained model (LoRA)
‚îÇ   ‚îî‚îÄ‚îÄ merged_model/                                 # Merged complete model
‚îÇ
‚îú‚îÄ‚îÄ README.md                                         # Project README
‚îú‚îÄ‚îÄ Ê®°ÂûãË®ìÁ∑¥ÊåáÂçó.md                                      # This file
‚îî‚îÄ‚îÄ Ê®°ÂûãÊ∏¨Ë©¶ÊåáÂçó.md                                      # Testing guide
```

## üéØ Training Pipeline

### Phase 1: SFT Pretraining (30-60 minutes)

**Purpose**: Warm-up model to learn JSON format and basic tool selection

```python
# Configuration
- Dataset: 800 examples
- Epochs: 1
- Batch size: 2 (effective 16 with gradient accumulation)
- Learning rate: 2e-4
```

**Expected Results**:
- JSON valid rate: ~95%
- Action accuracy: ~90%
- Model learns to output structured tool calls

### Phase 2: GRPO Training (2-4 hours)

**Purpose**: Refine policy through reinforcement learning

```python
# Configuration
- Dataset: 800 examples
- Epochs: 3
- Batch size: 2 (effective 16 with gradient accumulation)
- Learning rate: 5e-6
- Temperature: 0.7
```

**Reward Function**:
- ‚úÖ +1.0 per correct action name
- ‚úÖ +0.5 per correct argument set
- ‚úÖ +0.5 for correct ordering
- ‚ùå -0.3 per incorrect action
- ‚ùå -0.4 per missing required argument

**Expected Results**:
- Exact match rate: 70-85%
- Action F1 score: 75-90%
- Rewards increasing over training

## üìä Evaluation Metrics

The pipeline tracks comprehensive metrics:

### Core Metrics
- **Exact Match Rate**: % of perfectly correct responses
- **Action Precision**: % of predicted actions that are correct
- **Action Recall**: % of expected actions that were predicted
- **Action F1 Score**: Harmonic mean of precision and recall

### Quality Metrics
- **JSON Parse Success**: % of valid JSON outputs
- **Ordering Accuracy**: % of correct action sequences
- **Completeness Score**: % of responses with all required arguments

### Example Output
```json
{
  "exact_match_rate": 0.78,
  "action_precision": 0.85,
  "action_recall": 0.82,
  "action_f1": 0.84,
  "json_parse_success": 0.95,
  "ordering_accuracy": 0.80
}
```

## üõ†Ô∏è Configuration

### Model Configuration

```python
# In notebook Cell 1-2
max_seq_length = 2048      # Context window
lora_rank = 16             # LoRA rank (higher = more capacity)
load_in_4bit = True        # 4-bit quantization
offload_embedding = True   # Reduce VRAM by 1GB
```

### Training Configuration

```python
# SFT Configuration (Cell 16)
SFTConfig(
    output_dir="outputs/tau_bench_rl/sft",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    warmup_steps=10,
)

# GRPO Configuration (Cell 18)
GRPOConfig(
    output_dir="outputs/tau_bench_rl",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=5e-6,
    max_completion_length=512,
    temperature=0.7,
)
```

### Dataset Configuration

```python
# In configs.py
dataset_path = "dataset.jsonl"  # 800 tasks
# Or use extended dataset:
dataset_path = "3.5k.jsonl"       # 3500+ tasks
```

## üîß Troubleshooting

### Common Issues

#### Issue 1: CUDA Out of Memory

**Symptoms**: `RuntimeError: CUDA out of memory`

**Solutions**:
```python
# Option 1: Reduce batch size
per_device_train_batch_size = 1

# Option 2: Increase gradient accumulation
gradient_accumulation_steps = 16

# Option 3: Reduce sequence length
max_seq_length = 1024

# Option 4: Enable more offloading
offload_embedding = True
```

#### Issue 2: Training Not Converging

**Symptoms**: Rewards not increasing, loss plateaus

**Solutions**:
```python
# Option 1: Adjust learning rate
learning_rate = 1e-5  # Try different values: 5e-6, 1e-5, 2e-5

# Option 2: Reduce temperature
temperature = 0.5  # More deterministic

# Option 3: Check dataset quality
# See ANALYSIS_REPORT.md for dataset issues
```

#### Issue 3: Low Accuracy (<60%)

**Symptoms**: Poor performance on evaluation

**Solutions**:
1. Check dataset quality (see ANALYSIS_REPORT.md)
2. Increase training data (use 1k.jsonl)
3. Simplify reward function
4. Allow reasoning in system prompt

#### Issue 4: Training Too Slow

**Symptoms**: >8 hours for completion

**Solutions**:
```python
# Option 1: Reduce epochs
num_train_epochs = 1  # For SFT
num_train_epochs = 2  # For GRPO

# Option 2: Use larger batch size (if VRAM allows)
per_device_train_batch_size = 4

# Option 3: Reduce dataset size for testing
# Use subset: rl_tasks[:100] for quick iteration
```

## üìö Dataset Format

### Input Format (dataset.jsonl)

Each line is a JSON object with:

```json
{
  "query": "Hi, I need to cancel order #W1234567. My email is john@example.com",
  "gold_actions": [
    {
      "name": "find_user_id_by_email",
      "arguments": {"email": "john@example.com"}
    },
    {
      "name": "cancel_pending_order",
      "arguments": {"order_id": "#W1234567", "reason": "customer request"}
    }
  ],
  "gold_final": "cancelled the pending order"
}
```

### Output Format (Model Response)

```json
{
  "tool_calls": [
    {
      "name": "find_user_id_by_email",
      "arguments": {"email": "john@example.com"}
    },
    {
      "name": "cancel_pending_order",
      "arguments": {"order_id": "#W1234567", "reason": "customer request"}
    }
  ]
}
```

## üéì Advanced Usage

### Custom Dataset

```python
# Create your own dataset
tasks = []
for item in your_data:
    task = {
        "query": item['customer_request'],
        "gold_actions": item['expected_tool_calls'],
        "gold_final": item['expected_outcome']
    }
    tasks.append(task)

# Save as JSONL
with open('custom_dataset.jsonl', 'w') as f:
    for task in tasks:
        f.write(json.dumps(task) + '\n')

# Update notebook to use custom dataset
dataset_path = "custom_dataset.jsonl"
```

### Hyperparameter Tuning

```python
# Grid search configuration
configs = [
    {"learning_rate": 5e-6, "temperature": 0.5},
    {"learning_rate": 1e-5, "temperature": 0.7},
    {"learning_rate": 2e-5, "temperature": 0.9},
]

for config in configs:
    trainer = GRPOTrainer(
        model=model,
        args=GRPOConfig(**config),
        ...
    )
    trainer.train()
    evaluate_and_log(config)
```

### Multi-GPU Training

```python
# Modify training args
training_args = GRPOConfig(
    ...
    per_device_train_batch_size=4,  # Per GPU
    gradient_accumulation_steps=4,
    dataloader_num_workers=4,
    ddp_find_unused_parameters=False,
)

# Launch with torchrun
# torchrun --nproc_per_node=4 train_script.py
```

## üêõ Known Issues

1. **Dataset Quality**: See ANALYSIS_REPORT.md for detailed issues
   - Address fields incomplete (city/state/zip empty)
   - Item IDs mismatch between query and gold actions
   - Some queries missing email information

2. **Training Instability**: With 700 samples, risk of overfitting
   - Recommend using 1k.jsonl or data augmentation
   - Consider early stopping on validation set

3. **Reward Function Complexity**: Multiple fallback patterns may cause reward hacking
   - Consider simplified reward function (strict JSON only)

## üìù Related Documentation

- [ANALYSIS_REPORT.md](ANALYSIS_REPORT.md) - Detailed analysis of current implementation
- [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md) - Technical comparison of training approaches
- [TRAINING_METHODOLOGY.md](TRAINING_METHODOLOGY.md) - Comprehensive training methodology guide

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

1. **Dataset Quality**: Fix address parsing, ID matching
2. **Data Augmentation**: Expand to 1000+ samples
3. **Reward Function**: Simplify and improve
4. **Evaluation**: Add more comprehensive metrics
5. **Documentation**: Add more examples and tutorials

## üìÑ License

[Add your license here]

## üôè Acknowledgments

- [Unsloth](https://github.com/unslothai/unsloth) - For efficient fine-tuning
- [TRL](https://github.com/huggingface/trl) - For GRPO implementation
- [Tau-Bench](https://github.com/sierra-research/tau-bench) - For benchmark framework
- GPT-OSS Team - For the base model

## üìû Support

For issues and questions:

1. Check [ANALYSIS_REPORT.md](ANALYSIS_REPORT.md) for common problems
2. Review the Troubleshooting section above
3. Open an issue on GitHub
4. Contact [your-email@example.com]

## üîÑ Version History

- **v1.0** (2025-01-07): Initial release with two-phase training
  - SFT pretraining + GRPO refinement
  - 700 sample dataset
  - Tool-level cumulative rewards
  - Unsloth optimization

## üö¶ Current Status

**Training Status**: ‚ö†Ô∏è In Progress  
**Model Performance**: üìä To be evaluated after training completes  
**Documentation**: ‚úÖ Complete  
**Known Issues**: ‚ö†Ô∏è See ANALYSIS_REPORT.md

---

**Last Updated**: 2025-01-07  
**Maintainer**: [Your Name]  
**Version**: 1.0.0
