# APIGen-MT-5k Task Generation & Testing System

<div align="center">

**A comprehensive framework for generating and testing multi-turn customer service tasks**

[ä¸­æ–‡æ–‡æª”](#ä¸­æ–‡æ–‡æª”) | [English Documentation](#english-documentation)

</div>

---

## ğŸ“‹ ç›®éŒ„ / Table of Contents

- [ä¸­æ–‡æ–‡æª”](#ä¸­æ–‡æ–‡æª”)
  - [å°ˆæ¡ˆæ¦‚è¿°](#å°ˆæ¡ˆæ¦‚è¿°)
  - [ç›®éŒ„çµæ§‹](#ç›®éŒ„çµæ§‹)
  - [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
  - [æ ¸å¿ƒçµ„ä»¶](#æ ¸å¿ƒçµ„ä»¶)
  - [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
  - [é…ç½®ç³»çµ±](#é…ç½®ç³»çµ±)
  - [æ¨¡å‹è¨“ç·´èˆ‡æ¸¬è©¦](#æ¨¡å‹è¨“ç·´èˆ‡æ¸¬è©¦)
  - [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
- [English Documentation](#english-documentation)
  - [Project Overview](#project-overview)
  - [Directory Structure](#directory-structure)
  - [Quick Start](#quick-start)
  - [Core Components](#core-components)
  - [Usage Examples](#usage-examples)
  - [Configuration System](#configuration-system)
  - [Model Training & Testing](#model-training--testing)
  - [Troubleshooting](#troubleshooting)

---

# ä¸­æ–‡æ–‡æª”

## å°ˆæ¡ˆæ¦‚è¿°

**APIGen-MT-5k** æ˜¯ä¸€å€‹å…ˆé€²çš„å¤šè¼ªä»»å‹™ç”Ÿæˆèˆ‡æ¸¬è©¦ç³»çµ±ï¼Œå°ˆç‚ºå®¢æˆ¶æœå‹™å ´æ™¯è¨­è¨ˆã€‚è©²ç³»çµ±å¯¦ç¾äº† **AgentFlow** å¤šè¼ªè¿­ä»£æ¶æ§‹ï¼Œæ”¯æ´é›™æ¨¡å‹å”ä½œï¼ˆGPT-4o + GPT-OSS-120Bï¼‰ï¼Œæä¾›å®Œæ•´çš„ä»»å‹™ç”Ÿæˆã€é©—è­‰ã€æ¸¬è©¦å’Œåˆ†ææµç¨‹ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **ğŸ”„ AgentFlow æ¶æ§‹**: å¤šè¼ªè¿­ä»£ç”Ÿæˆï¼Œé€šéè¦åŠƒå™¨ã€åŸ·è¡Œå™¨ã€é©—è­‰å™¨å’Œç”Ÿæˆå™¨å”ä½œï¼Œç”Ÿæˆé«˜è³ªé‡ä»»å‹™
- **ğŸ¤– é›™æ¨¡å‹ç³»çµ±**: GPT-4oï¼ˆç”¨æˆ¶æ¨¡å‹ï¼‰+ GPT-OSS-120Bï¼ˆåŠ©æ‰‹æ¨¡å‹ï¼‰å”ä½œï¼Œæå‡ä»»å‹™è³ªé‡
- **âœ… æ™ºèƒ½é©—è­‰**: è‡ªå‹•é©—è­‰ä»»å‹™çš„æ•¸æ“šä¸€è‡´æ€§ã€ç”¨æˆ¶IDä¸€è‡´æ€§å’Œå·¥å…·èª¿ç”¨æ­£ç¢ºæ€§
- **ğŸ“Š å…¨é¢æ¸¬è©¦**: æ”¯æ´ä¸¦è¡Œæ¸¬è©¦ã€å¤šç¨®è©•ä¼°æŒ‡æ¨™å’Œçµæœå¯è¦–åŒ–
- **ğŸ”§ éˆæ´»é…ç½®**: é€šé `configs.py` é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®åƒæ•¸
- **ğŸ“ˆ è©³ç´°å ±å‘Š**: ç”Ÿæˆ JSONã€CSV å’Œå¯è¦–åŒ–å ±å‘Šï¼Œæ”¯æ´æ·±åº¦åˆ†æ
- **ğŸ“ æ¨¡å‹è¨“ç·´**: æ”¯æ´ GPT-OSS 20B æ¨¡å‹çš„å¼·åŒ–å­¸ç¿’è¨“ç·´ï¼ˆGRPO + SFTï¼‰ï¼Œå¯è‡ªå®šç¾©è¨“ç·´å®¢æœä»»å‹™æ¨¡å‹

### ğŸ—ï¸ ç³»çµ±æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APIGen-MT-5k System                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Task Pipelineâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚Task Generatorâ”‚        â”‚
â”‚  â”‚  (Pipeline)  â”‚                  â”‚ (AgentFlow)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                 â”‚                â”‚
â”‚         â”‚                                 â†“                â”‚
â”‚         â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚         â”‚                          â”‚  Validator   â”‚        â”‚
â”‚         â”‚                          â”‚ (Consistency â”‚        â”‚
â”‚         â”‚                          â”‚   Checker)   â”‚        â”‚
â”‚         â”‚                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                 â”‚                â”‚
â”‚         â†“                                 â†“                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Task Tester  â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚Generated Taskâ”‚        â”‚
â”‚  â”‚ (Dual Model) â”‚                  â”‚    (JSON)    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                                  â”‚
â”‚         â†“                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚   Results    â”‚                                          â”‚
â”‚  â”‚ (JSON/CSV/   â”‚                                          â”‚
â”‚  â”‚Visualization)â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ç›®éŒ„çµæ§‹

```
APIGen-MT-5k/
â”œâ”€â”€ task_tester.py              # ä»»å‹™æ¸¬è©¦å™¨ï¼ˆæ”¯æ´é›™æ¨¡å‹ï¼‰
â”œâ”€â”€ task_generator.py           # ä»»å‹™ç”Ÿæˆå™¨ï¼ˆæ”¯æ´ AgentFlowï¼‰
â”œâ”€â”€ task_pipeline.py            # ä»»å‹™ç”Ÿæˆç®¡é“
â”œâ”€â”€ configs.py                  # é…ç½®ç®¡ç†
â”œâ”€â”€ data_reader.py              # æ•¸æ“šè®€å–å·¥å…·
â”‚
â”œâ”€â”€ envs/                       # ç’°å¢ƒæ•¸æ“š
â”‚   â””â”€â”€ retail/                 # é›¶å”®ç’°å¢ƒ
â”‚       â”œâ”€â”€ users.json          # ç”¨æˆ¶æ•¸æ“š
â”‚       â”œâ”€â”€ orders.json         # è¨‚å–®æ•¸æ“š
â”‚       â”œâ”€â”€ products.json       # ç”¢å“æ•¸æ“š
â”‚       â””â”€â”€ tools/              # å·¥å…·å¯¦ç¾
â”‚
â”œâ”€â”€ generated_tasks/            # ç”Ÿæˆçš„ä»»å‹™
â”‚   â”œâ”€â”€ Sampled_Tasks.json
â”‚   â”œâ”€â”€ test_agentflow.json
â”‚   â””â”€â”€ test_direct.json
â”‚
â”œâ”€â”€ results/                    # æ¸¬è©¦çµæœ
â”‚   â”œâ”€â”€ test_results_*.json
â”‚   â”œâ”€â”€ test_summary_*.json
â”‚   â”œâ”€â”€ test_summary_*.csv
â”‚   â””â”€â”€ test_visualization_*.png
â”‚
â”œâ”€â”€ scripts/                    # è¼”åŠ©è…³æœ¬
â”‚   â”œâ”€â”€ analysis_helpers.py              # åˆ†æè¼”åŠ©å‡½æ•¸åº«
â”‚   â”œâ”€â”€ analyze_generated_tasks.py       # åˆ†æç”Ÿæˆä»»å‹™çš„çµ±è¨ˆä¿¡æ¯
â”‚   â”œâ”€â”€ analyze_real_data_references.py  # åˆ†æçœŸå¯¦æ•¸æ“šå¼•ç”¨æƒ…æ³
â”‚   â”œâ”€â”€ analyze_successful_tasks.py      # åˆ†ææˆåŠŸä»»å‹™çš„æ¨¡å¼
â”‚   â”œâ”€â”€ analyze_unsuccessful_tasks.py     # åˆ†æå¤±æ•—ä»»å‹™çš„æ¨¡å¼
â”‚   â”œâ”€â”€ compare_blueprint_agentflow.py   # æ¯”è¼ƒå…©ç¨®ç”Ÿæˆæ–¹æ³•
â”‚   â”œâ”€â”€ filter_successful_sample_tasks.py # éæ¿¾æˆåŠŸå’Œå¤±æ•—çš„ä»»å‹™
â”‚   â””â”€â”€ find_specific_failed_tasks.py    # æŸ¥æ‰¾ç‰¹å®šå¤±æ•—ä»»å‹™
â”‚
â”œâ”€â”€ utils/                      # å·¥å…·å‡½æ•¸
â”‚   â”œâ”€â”€ arg_normalizer.py       # åƒæ•¸æ¨™æº–åŒ–å·¥å…·
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ gpt_oss_20b_tau_bench_rl_training.ipynb  # æ¨¡å‹è¨“ç·´ notebook
â”œâ”€â”€ dataset.jsonl               # è¨“ç·´æ•¸æ“šé›†ï¼ˆ800ä»»å‹™ï¼‰
â”œâ”€â”€ 3.5k.jsonl                  # æ“´å±•æ•¸æ“šé›†ï¼ˆ3500+ä»»å‹™ï¼‰
â”œâ”€â”€ server.py                   # æ¨¡å‹éƒ¨ç½²æœå‹™å™¨ï¼ˆFastAPIï¼‰
â”œâ”€â”€ run_training.sh             # è¨“ç·´è…³æœ¬
â”œâ”€â”€ dgx_train.slurm             # SLURM è¨“ç·´é…ç½®
â”œâ”€â”€ training_configs.ini        # è¨“ç·´é…ç½®æ–‡ä»¶
â”‚
â”œâ”€â”€ README.md                   # æœ¬æ–‡æª”
â”œâ”€â”€ æ¨¡å‹è¨“ç·´æŒ‡å—.md              # æ¨¡å‹è¨“ç·´å®Œæ•´æŒ‡å—
â”œâ”€â”€ æ¨¡å‹æ¸¬è©¦æŒ‡å—.md              # æ¨¡å‹æ¸¬è©¦éƒ¨ç½²æŒ‡å—
â”œâ”€â”€ AGENTFLOW_README.md        # AgentFlow è©³ç´°æ–‡æª”
â”œâ”€â”€ Compare_agentflow_direct.md # æ–¹æ³•å°æ¯”åˆ†æ
â””â”€â”€  SUMMARY.md                  # å°ˆæ¡ˆæ‘˜è¦
```

## å¿«é€Ÿé–‹å§‹

### 1. å®‰è£ä¾è³´

```bash
# å¿…éœ€ä¾è³´
pip install openai

# å¯é¸ä¾è³´ï¼ˆç”¨æ–¼é€²åº¦æ¢å’Œå¯è¦–åŒ–ï¼‰
pip install tqdm pandas matplotlib
```

### 2. é…ç½® API å¯†é‘°

ç·¨è¼¯ `configs.py` æ–‡ä»¶ï¼Œè¨­ç½®ä½ çš„ API å¯†é‘°ï¼š

```python
@dataclass
class TauBenchConfig:
    # GPT-4o (ç”¨æˆ¶æ¨¡å‹)
    user_model: str = "gpt-4o"
    user_api_key: str = "your-gpt4o-api-key"
    user_base_url: str = "https://api.openai.com/v1"
    
    # GPT-OSS-120B (åŠ©æ‰‹æ¨¡å‹)
    default_model: str = "vllm-a40-gpt-oss-120b"
    default_api_key: str = "your-gpt-oss-api-key"
    default_base_url: str = "https://your-api-endpoint/api"
```

### 3. åŸºæœ¬ä½¿ç”¨

```bash
# ç”Ÿæˆä»»å‹™ï¼ˆAgentFlow æ¨¡å¼ï¼‰
python task_pipeline.py --num-tasks 10 --agentflow --output generated_tasks/my_tasks.json

# æ¸¬è©¦ä»»å‹™ï¼ˆé›™æ¨¡å‹ï¼‰
python task_tester.py --tasks generated_tasks/my_tasks.json --dual-model --verbose --save-results --visualize

# æŸ¥çœ‹çµæœ
ls results/
```

## æ ¸å¿ƒç»„ä»¶

### 1. Task Tester (`task_tester.py`)

ä»»å‹™æ¸¬è©¦å™¨ï¼Œæ”¯æŒé›™æ¨¡å‹æ¸¬è©¦å™¨ï¼Œæ”¯æŒé›™æ¨¡å‹å”ä½œå’Œå¤šç¨®æ¸¬è©¦æ¨¡å¼ã€‚

#### ä¸»è¦åŠŸèƒ½

- âœ… **é›™æ¨¡å‹æ¸¬è©¦**: GPT-4o å¢å¼·æŸ¥è©¢ + GPT-OSS-120B åŸ·è¡Œä»»å‹™
- âœ… **ä¸¦è¡Œæ¸¬è©¦**: å¤šç·šç¨‹ä¸¦è¡Œè™•ç†ï¼Œæå‡æ¸¬è©¦æ•ˆç‡
- âœ… **å¤šç¨®æ¨¡å¼**: çœŸå¯¦åŸ·è¡Œã€æ¨¡æ“¬åŸ·è¡Œã€åƒ…é©—è­‰
- âœ… **è©³ç´°æŒ‡æ¨™**: ç²¾ç¢ºåº¦ã€å¬å›ç‡ã€F1åˆ†æ•¸ã€è¼¸å‡ºåŒ¹é…ç‡ç­‰
- âœ… **çµæœå¯è¦–åŒ–**: è‡ªå‹•ç”Ÿæˆåœ–è¡¨å’Œçµ±è¨ˆå ±å‘Š

#### å‘½ä»¤è¡Œåƒæ•¸

**è¼¸å…¥é¸é …**:
```bash
--tasks FILE                    # è¦æ¸¬è©¦çš„ä»»å‹™ JSON æ–‡ä»¶
```

**é›™æ¨¡å‹é¸é …**:
```bash
--dual-model                    # å•Ÿç”¨é›™æ¨¡å‹æ–¹æ³•
--enhance-query                 # ä½¿ç”¨ç”¨æˆ·æ¨¡å‹å¢å¼ºæŸ¥è¯¢
```

**åŠ©æ‰‹æ¨¡å‹é…ç½®** (GPT-OSS-120B):
```bash
--model MODEL                   # æ¨¡å‹åç¨±
--api-key KEY                   # API å¯†é‘°
--base-url URL                  # API åŸºç¤URL
```

**ç”¨æˆ·æ¨¡å‹é…ç½®** (GPT-4o):
```bash
--user-model MODEL              # ç”¨æˆ·æ¨¡å‹åç¨±
--user-api-key KEY              # ç”¨æˆ·æ¨¡å‹ API å¯†é‘°
--user-base-url URL             # ç”¨æˆ·æ¨¡å‹ API åŸºç¤URL
```

**å·¥å…·é…ç½®**:
```bash
--envs-path PATH                # é›¶å”®ç’°å¢ƒè·¯å¾‘ï¼ˆé»˜èª: envs/retailï¼‰
```

**åŸ·è¡Œé¸é …**:
```bash
--threads N                     # ä¸¦è¡Œç·šç¨‹æ•¸ï¼ˆé»˜èª: 1ï¼‰
--verbose                       # è©³ç´°è¼¸å‡º
--dry-run                       # æ¨¡æ“¬é‹è¡Œï¼ˆä¸èª¿ç”¨ LLMï¼‰
--validate-only                 # åƒ…é©—è­‰ä»»å‹™ï¼ˆä¸èª¿ç”¨æ¨¡å‹ï¼‰
```

**è¼¸å‡ºé¸é …**:
```bash
--output-dir DIR                # çµæœä¿å­˜ç›®éŒ„ï¼ˆé»˜èª: resultsï¼‰
--save-results                  # ä¿å­˜è©³ç´°çµæœ
--visualize                     # ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨
```

#### ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºç¤æ¸¬è©¦
python task_tester.py --tasks generated_tasks/my_tasks.json --verbose

# é›™æ¨¡å‹æ¸¬è©¦ + å¯è¦–åŒ–
python task_tester.py \
    --tasks generated_tasks/my_tasks.json \
    --dual-model \
    --enhance-query \
    --threads 4 \
    --save-results \
    --visualize

# åƒ…é©—è­‰ï¼ˆä¸èª¿ç”¨æ¨¡å‹ï¼‰
python task_tester.py --tasks generated_tasks/my_tasks.json --validate-only

# æ¨¡æ“¬é‹è¡Œï¼ˆå¿«é€Ÿæ¸¬è©¦ï¼‰
python task_tester.py --tasks generated_tasks/my_tasks.json --dry-run --verbose
```

### 2. Task Generator (`task_generator.py`)

ä»»å‹™ç”Ÿæˆå™¨ï¼Œæ”¯æ´ AgentFlow å¤šè¼ªè¿­ä»£ç”Ÿæˆå’Œç›´æ¥ç”Ÿæˆå…©ç¨®æ¨¡å¼ã€‚

#### AgentFlow æ¶æ§‹

AgentFlow æ˜¯ä¸€å€‹å¤šè¼ªè¿­ä»£ç”Ÿæˆæ¶æ§‹ï¼ŒåŒ…å«ä»¥ä¸‹çµ„ä»¶ï¼š

```
Turn t: Query (q) + Knowledge (K) + Memory (M^t)
    â†“
  Planner (Ï€_Î¸) â†’ Actions (a^t)
    â†“
  Executor â†’ Commands (c^t) + Results (e^t)
    â†“
  Verifier â†’ Analysis + Status (v^t)
    â†“
  Generator (if complete) â†’ Answer (o)
    â†“
  Memory^(t+1) (accumulated context)
```

**çµ„ä»¶èªªæ˜**:

1. **Plannerï¼ˆè¦åŠƒå™¨ï¼‰**: åˆ†ææŸ¥è©¢å’Œä¸Šä¸‹æ–‡ï¼Œåˆ¶å®šè¡Œå‹•è¨ˆåŠƒ
2. **Executorï¼ˆåŸ·è¡Œå™¨ï¼‰**: åŸ·è¡Œè¨ˆåŠƒçš„å‹•ä½œï¼Œç²å–çµæœ
3. **Verifierï¼ˆé©—è­‰å™¨ï¼‰**: é©—è­‰åŸ·è¡Œçµæœï¼Œæ±ºå®šä¸‹ä¸€æ­¥è¡Œå‹•
4. **Generatorï¼ˆç”Ÿæˆå™¨ï¼‰**: åœ¨ä»»å‹™å®Œæˆæ™‚ç”Ÿæˆæœ€çµ‚è¼¸å‡º
5. **Memoryï¼ˆå…§å­˜ï¼‰**: è·¨è¼ªæ¬¡ç´¯ç©ä¸Šä¸‹æ–‡ä¿¡æ¯

#### ä½¿ç”¨ç¤ºä¾‹

```python
from task_generator import TauBenchOpenAIGenerator

# å‰µå»ºç”Ÿæˆå™¨
generator = TauBenchOpenAIGenerator("envs/retail")

# ä½¿ç”¨ AgentFlow ç”Ÿæˆå–®å€‹ä»»å‹™
result = generator.generate_task_with_agentflow(
    max_turns=5,
    include_metadata=True
)

# æ‰¹é‡ç”Ÿæˆä»»å‹™
tasks = generator.generate_diverse_tasks(
    num_tasks=10,
    use_agentflow=True
)

# ä¿å­˜ä»»åŠ¡
generator.save_tasks_to_file(tasks, "generated_tasks/my_tasks.json")
```

è©³ç´°çš„ AgentFlow æ–‡æª”è¯·å‚è€ƒ [AGENTFLOW_README.md](AGENTFLOW_README.md)

### 3. Task Pipeline (`task_pipeline.py`)

ä»»å‹™ç”Ÿæˆç®¡é“ï¼Œé›†æˆäº†ç”Ÿæˆã€é©—è­‰ã€å¯©æŸ¥å’Œä¼˜=å„ªåŒ–çš„å®Œæ•´æµç¨‹ã€‚

#### ä¸»è¦åŠŸèƒ½

- ğŸ”„ **è¿­ä»£å„ªåŒ–**: è‡ªå‹•é€²è¡Œå¤šæ¬¡è¿­ä»£ï¼Œç›´åˆ°ä»»å‹™é€šéé©—è­‰
- ğŸ‘¥ **å¯©æŸ¥å§”å“¡æœƒ**: å¤šå€‹å¯©æŸ¥å™¨è©•ä¼°ä»»å‹™è³ªé‡
- ğŸ” **ç”¨æˆ¶IDé©—è­‰**: ç¢ºä¿æ‰€æœ‰å‹•ä½œä½¿ç”¨ä¸€è‡´çš„ç”¨æˆ¶ID
- ğŸ“Š **çµ±è¨ˆå ±å‘Š**: è©³ç´°çš„ç”Ÿæˆçµ±è¨ˆå’Œå¤±æ•—åŸå› åˆ†æ
- ğŸ¯ **å ´æ™¯å¤šæ¨£æ€§**: ç¢ºä¿ç”Ÿæˆä»»å‹™è¦†è“‹å¤šç¨®å ´æ™¯é¡å‹

#### å‘½ä»¤è¡Œåƒæ•¸

```bash
--num-tasks N                   # ç”Ÿæˆä»»å‹™æ•¸é‡ï¼ˆé»˜èª: 3ï¼‰
--max-iterations N              # æ¯ä¸ªä»»å‹™æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼ˆé»˜èª: 3ï¼‰
--output FILE                   # è¼¸å‡ºæ–‡ä»¶è·¯å¾‘ï¼ˆé»˜èª: generated_tasks/Sampled_Tasks.jsonï¼‰
--no-user-id-validation         # ç¦ç”¨ç”¨æˆ·IDä¸€è‡´æ€§é©—è­‰
--committee-size N              # å¯©æŸ¥å§”å“¡æœƒå¤§å°ï¼ˆé»˜èª: 3ï¼‰
--agentflow                     # ä½¿ç”¨ AgentFlow å¤šè¼ªç”Ÿæˆ
--agentflow-turns N             # AgentFlow æœ€å¤§è¼ªæ•¸ï¼ˆé»˜èª: 5ï¼‰
```

#### ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºç¤ç”Ÿæˆï¼ˆç›´æ¥æ¨¡å¼ï¼‰
python task_pipeline.py --num-tasks 10 --output generated_tasks/my_tasks.json

# ä½¿ç”¨ AgentFlow ç”Ÿæˆ
python task_pipeline.py --num-tasks 10 --agentflow --agentflow-turns 5 --output generated_tasks/agentflow_tasks.json

# å¢åŠ è¿­ä»£æ¬¡æ•¸å’Œå§”å“¡æœƒå¤§å°
python task_pipeline.py --num-tasks 5 --max-iterations 5 --committee-size 5 --output generated_tasks/high_quality_tasks.json

# ç¦ç”¨ç”¨æˆ¶IDé©—è­‰ï¼ˆåƒ…åœ¨ç‰¹æ®Šæƒ…æ³ä¸‹ä½¿ç”¨ï¼‰
python task_pipeline.py --num-tasks 10 --no-user-id-validation --output generated_tasks/tasks.json
```

#### ç®¡é“æµç¨‹

```
1. åˆå§‹åŒ–
   â†“
2. ç”Ÿæˆä»»å‹™ï¼ˆAgentFlow/Directï¼‰
   â†“
3. æ•¸æ“šé©—è­‰
   â”œâ”€ ç”¨æˆ¶å­˜åœ¨æ€§
   â”œâ”€ è¨‚å–®å­˜åœ¨æ€§
   â”œâ”€ ç”¨æˆ¶-è¨‚å–®åŒ¹é…
   â””â”€ å·¥å…·åƒæ•¸æœ‰æ•ˆæ€§
   â†“
4. å¯©æŸ¥å§”å“¡æœƒè©•å¯©
   â”œâ”€ å¯©æŸ¥å™¨ 1
   â”œâ”€ å¯©æŸ¥å™¨ 2
   â””â”€ å¯©æŸ¥å™¨ 3
   â†“
5. æ±ºç­–
   â”œâ”€ é€šé â†’ ä¿å­˜ä»»å‹™
   â”œâ”€ æœªé€šé â†’ ç”Ÿæˆåé¥‹
   â””â”€ é‡æ–°ç”Ÿæˆï¼ˆæœ€å¤š max_iterations æ¬¡ï¼‰
   â†“
6. çµ±è¨ˆå ±å‘Š
```

### 4. Configuration System (`configs.py`)

é›†ä¸­å¼é…ç½®ç®¡ç†ç³»çµ±ã€‚

#### ä¸»è¦é…ç½®é …

```python
@dataclass
class TauBenchConfig:
    # ç’°å¢ƒè·¯å¾‘
    envs_path: str = "envs/retail"
    
    # æ¨¡å‹é…ç½®
    user_model: str = "gpt-4o"                    # ç”¨æˆ¶æ¨¡å‹
    user_api_key: str = "your-key"               # ç”¨æˆ¶æ¨¡å‹å¯†é‘°
    user_base_url: str = "https://api.openai.com/v1"
    
    default_model: str = "vllm-a40-gpt-oss-120b" # åŠ©æ‰‹æ¨¡å‹
    default_api_key: str = "your-key"            # åŠ©æ‰‹æ¨¡å‹å¯†é‘°
    default_base_url: str = "https://your-endpoint/api"
    
    # ç”Ÿæˆé…ç½®
    num_tasks: int = 10                          # é»˜èªä»»å‹™æ•¸é‡
    temperature: float = 0.0                     # æº«åº¦åƒæ•¸
    max_tokens: int = 16384                      # æœ€å¤§ä»¤ç‰Œæ•¸
    max_retries: int = 3                         # æœ€å¤§é‡è©¦æ¬¡æ•¸
    timeout: int = 180                           # è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
    
    # é©—è­‰é…ç½®
    test_timeout: int = 180                      # æ¸¬è©¦è¶…æ™‚
    max_workers: int = 2                         # æœ€å¤§å·¥ä½œç·šç¨‹
    
    # è¼¸å‡ºé…ç½®
    output_dir: str = "tau_bench_results"        # è¼¸å‡ºç›®éŒ„
    save_visualizations: bool = True             # ä¿å­˜å¯è¦–åŒ–
    
    # è³ªé‡æŒ‡æ¨™é–¾å€¼
    min_success_rate: float = 0.7                # æœ€ä½æˆåŠŸç‡
    min_action_recall: float = 0.6               # æœ€ä½å‹•ä½œå¬å›ç‡
    min_action_precision: float = 0.6            # æœ€ä½å‹•ä½œç²¾ç¢ºç‡
    
    # å ´æ™¯é…ç½®
    scenario_keys: List[str] = [                 # å ´æ™¯é¡å‹
        'order_cancellation',
        'order_modification',
        'item_return',
        'item_exchange',
        'address_change',
        'payment_update',
        'order_inquiry',
        'product_inquiry',
    ]
```

## ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´å·¥ä½œæµ

```bash
# æ­¥é©Ÿ 1: ä½¿ç”¨ AgentFlow ç”Ÿæˆé«˜è³ªé‡ä»»åŠ¡
python task_pipeline.py \
    --num-tasks 20 \
    --agentflow \
    --agentflow-turns 5 \
    --max-iterations 3 \
    --committee-size 3 \
    --output generated_tasks/production_tasks.json

# æ­¥é©Ÿ 2: ä½¿ç”¨é›™æ¨¡å‹æ¸¬è©¦ä»»å‹™
python task_tester.py \
    --tasks generated_tasks/production_tasks.json \
    --dual-model \
    --enhance-query \
    --threads 8 \
    --save-results \
    --visualize \
    --output-dir results/production_test

# æ­¥éª¤ 3: æŸ¥çœ‹ç»“æœ
ls results/production_test/
# test_results_*.json      - è©³ç´°æ¸¬è©¦çµæœ
# test_summary_*.json      - å½™ç¸½å ±å‘Š
# test_summary_*.csv       - CSV æ ¼å¼
# test_visualization_*.png - å¯è¦–åŒ–åœ–è¡¨
```

### å°æ¯” AgentFlow vs Direct æ¨¡å¼

```bash
# ä½¿ç”¨è…³æœ¬å°æ¯”å…©ç¨®æ–¹æ³•
python scripts/compare_blueprint_agentflow.py \
    --num-tasks 10 \
    --direct-out generated_tasks/test_direct.json \
    --agent-out generated_tasks/test_agentflow.json \
    --agentflow-turns 5
```

è©³ç´°çš„å°æ¯”åˆ†æè«‹åƒè€ƒ [Compare_agentflow_direct.md](Compare_agentflow_direct.md)

### åˆ†ææ•°æ®å¼•ç”¨

```bash
# åˆ†æä»»å‹™ä¸­çš„çœŸå¯¦æ•¸æ“šå¼•ç”¨
python scripts/analyze_real_data_references.py \
    generated_tasks/my_tasks.json \
    --envs-path envs/retail \
    --output analysis_report.json
```

## é…ç½®ç³»çµ±

### ä¿®æ”¹é…ç½®

ä½ å¯ä»¥é€šéä»¥ä¸‹æ–¹å¼ä¿®æ”¹é…ç½®ï¼š

1. **ç›´æ¥ç·¨è¼¯ `configs.py`**ï¼ˆæ¨è–¦ç”¨æ–¼æ°¸ä¹…æ›´æ”¹ï¼‰
2. **å‘½ä»¤è¡Œåƒæ•¸è¦†è“‹**ï¼ˆç”¨æ–¼è‡¨æ™‚æ›´æ”¹ï¼‰
3. **ç’°å¢ƒè®Šé‡**ï¼ˆç”¨æ–¼æ•æ„Ÿä¿¡æ¯ï¼‰

### é…ç½®å„ªå…ˆç´š
```
å‘½ä»¤è¡Œåƒæ•¸ > configs.py è¨­ç½® > é»˜èªå€¼
```

### ç¤ºä¾‹ï¼šè‡ªå®šç¾©é…ç½®

```python
# åœ¨ä»£ç¢¼ä¸­ä½¿ç”¨è‡ªå®šç¾©é…ç½®
from configs import TauBenchConfig

config = TauBenchConfig()
config.num_tasks = 50
config.temperature = 0.7
config.max_workers = 4

# ä½¿ç”¨è‡ªå®šç¾©é…ç½®
from task_pipeline import TaskConfigurationPipeline, PipelineConfig

pipeline_config = PipelineConfig(
    envs_path=config.envs_path,
    max_iterations=5,
    committee_size=5,
    use_agentflow=True
)

pipeline = TaskConfigurationPipeline(pipeline_config)
```

## è¼¸å‡ºèªªæ˜

### 1. ä»»å‹™æ–‡ä»¶ (JSON)

```json
[
  {
    "success": true,
    "task": {
      "q": "æˆ‘éœ€è¦å–æ¶ˆè¨‚å–® #W123456",
      "agt": [
        {
          "name": "find_user_id_by_email",
          "arguments": {"email": "user@example.com"}
        },
        {
          "name": "get_order_details",
          "arguments": {"user_id": "U123", "order_id": "#W123456"}
        },
        {
          "name": "cancel_pending_order",
          "arguments": {"user_id": "U123", "order_id": "#W123456"}
        }
      ],
      "ogt": ["æ‚¨çš„è¨‚å–® #W123456 å·²æˆåŠŸå–æ¶ˆ"]
    },
    "metadata": {
      "generation_method": "agentflow",
      "turns": 3,
      "confidence": 0.95
    },
    "validation_report": {
      "valid": true,
      "missing": [],
      "suggestions": []
    }
  }
]
```

### 2. æ¸¬è©¦çµæœ (JSON)

```json
{
  "summary": {
    "total_tasks": 20,
    "successful": 18,
    "success_rate": 0.90,
    "avg_action_precision": 0.92,
    "avg_action_recall": 0.88,
    "avg_action_f1": 0.90,
    "exact_action_matches": 15
  },
  "results": [
    {
      "task_id": "task_001",
      "success": true,
      "action_precision": 1.0,
      "action_recall": 0.85,
      "action_f1": 0.92,
      "execution_time": 3.45,
      "model_response_time": 2.10,
      "tool_execution_time": 1.35
    }
  ]
}
```

### 3. CSV å½™ç¸½

| task_id | success | action_precision | action_recall | action_f1 | execution_time |
|---------|---------|------------------|---------------|-----------|----------------|
| task_001| True    | 1.00             | 0.85          | 0.92      | 3.45           |
| task_002| True    | 0.90             | 0.95          | 0.92      | 4.12           |

### 4. å¯è¦–åŒ–åœ–è¡¨

- **æˆåŠŸç‡åˆ†æ**: é¤…åœ–é¡¯ç¤ºæˆåŠŸ/å¤±æ•—ä»»å‹™æ¯”ä¾‹
- **æ€§èƒ½æŒ‡æ¨™**: æŸ±ç‹€åœ–é¡¯ç¤ºç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1åˆ†æ•¸
- **æ™‚é–“åˆ†ä½ˆ**: ç›´æ–¹åœ–é¡¯ç¤ºåŸ·è¡Œæ™‚é–“åˆ†ä½ˆ
- **å‹•ä½œåŒ¹é…**: ç†±åŠ›åœ–é¡¯ç¤ºå‹•ä½œåŒ¹é…æƒ…æ³

## æ¨¡å‹è¨“ç·´èˆ‡æ¸¬è©¦

æœ¬å°ˆæ¡ˆæ”¯æ´ **GPT-OSS 20B** æ¨¡å‹çš„å¼·åŒ–å­¸ç¿’è¨“ç·´ï¼Œå¯è‡ªå®šç¾©è¨“ç·´å®¢æœä»»å‹™æ¨¡å‹ã€‚è¨“ç·´æ¡ç”¨å…©éšæ®µæµç¨‹ï¼š

### ğŸ“š å®Œæ•´æ–‡æª”

- **[æ¨¡å‹è¨“ç·´æŒ‡å—.md](æ¨¡å‹è¨“ç·´æŒ‡å—.md)** - è©³ç´°çš„æ¨¡å‹è¨“ç·´æ•™ç¨‹
- **[æ¨¡å‹æ¸¬è©¦æŒ‡å—.md](æ¨¡å‹æ¸¬è©¦æŒ‡å—.md)** - æ¨¡å‹éƒ¨ç½²å’Œæ¸¬è©¦æŒ‡å—

### ğŸ› ï¸ è¨“ç·´ç’°å¢ƒè¨­ç½®

è¨“ç·´ç’°å¢ƒæ”¯æŒå¤šç¨®é…ç½®ï¼Œé¸æ“‡é©åˆæ‚¨çš„è¨­ç½®ï¼š

#### æœ¬åœ°ç’°å¢ƒï¼ˆæ¨è–¦ç”¨æ–¼é–‹ç™¼ï¼‰

```bash
# æ¨™æº–å®‰è£
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes

# é‡å° A100/H100 GPUï¼ˆæ›´å¥½æ€§èƒ½ï¼‰
pip install "unsloth[cu121-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

#### NVIDIA DGX Sparkï¼ˆå¤§è¦æ¨¡è¨“ç·´ï¼‰

ç”¨æ–¼è¨“ç·´ 200B åƒæ•¸ä»¥ä¸‹çš„å¤§å‹æ¨¡å‹ï¼ˆå¦‚ gpt-oss-120bï¼‰ï¼š

```bash
# 1. ä¸‹è¼‰ä¸¦æ§‹å»º Docker é¡åƒ
wget -O Dockerfile "https://raw.githubusercontent.com/unslothai/notebooks/main/Dockerfile_DGX_Spark"
docker build -f Dockerfile -t unsloth-dgx-spark .

# 2. å•Ÿå‹•å®¹å™¨
docker run -it --gpus=all --net=host --ipc=host \
    -v $(pwd):$(pwd) -w $(pwd) unsloth-dgx-spark

# 3. åœ¨å®¹å™¨å…§è¨“ç·´
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root
```

**å…§å­˜éœ€æ±‚**ï¼š
- GPT-OSS 20B: 24-40 GB VRAMï¼ˆ4-bit é‡åŒ–ï¼‰
- GPT-OSS 120B: ~68 GB çµ±ä¸€å…§å­˜ï¼ˆQLoRA 4-bitï¼‰

**è³‡æº**ï¼š
- ğŸ“š [Unsloth DGX Spark æ–‡æª”](https://unsloth.ai/docs/basics/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth)
- ğŸ³ [DGX Spark Dockerfile](https://raw.githubusercontent.com/unslothai/notebooks/main/Dockerfile_DGX_Spark)

#### é›²å¹³å°

**Google Colab**ï¼ˆå…è²» T4 GPUï¼‰ï¼š
```python
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
```

**AWS/Azure/GCP**ï¼ˆA100/H100ï¼‰ï¼š
```bash
pip install "unsloth[cu121-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

**è©³ç´°è¨­ç½®èªªæ˜è«‹åƒè€ƒ** [æ¨¡å‹è¨“ç·´æŒ‡å—.md](æ¨¡å‹è¨“ç·´æŒ‡å—.md)

### ğŸ“ è¨“ç·´æµç¨‹æ¦‚è¦½

#### ç¬¬ä¸€éšæ®µï¼šSFT é è¨“ç·´ï¼ˆ30-60 åˆ†é˜ï¼‰

ä½¿ç”¨ç›£ç£å¼å¾®èª¿è®“æ¨¡å‹å­¸ç¿’ JSON æ ¼å¼å’ŒåŸºæœ¬å·¥å…·é¸æ“‡ï¼š

```bash
# åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹
jupyter notebook gpt_oss_20b_tau_bench_rl_training.ipynb

# åŸ·è¡Œ Cell 1-16ï¼ˆSFT è¨“ç·´ï¼‰
```

**ç‰¹é»**ï¼š
- ä½¿ç”¨ 800 å€‹æ¨™è¨»æ¨£æœ¬
- å­¸ç¿’æ­£ç¢ºçš„å·¥å…·èª¿ç”¨æ ¼å¼
- JSON æœ‰æ•ˆç‡é” ~95%
- å‹•ä½œæº–ç¢ºç‡é” ~90%

#### ç¬¬äºŒéšæ®µï¼šGRPO è¨“ç·´ï¼ˆ2-4 å°æ™‚ï¼‰

ä½¿ç”¨å¼·åŒ–å­¸ç¿’å„ªåŒ–ç­–ç•¥ï¼š

```bash
# ç¹¼çºŒåŸ·è¡Œ Cell 17-21ï¼ˆGRPO è¨“ç·´ï¼‰
```

**çå‹µæ©Ÿåˆ¶**ï¼š
- âœ… +1.0 æ¯å€‹æ­£ç¢ºçš„å‹•ä½œåç¨±
- âœ… +0.5 æ¯å€‹æ­£ç¢ºçš„åƒæ•¸é›†
- âœ… +0.5 æ­£ç¢ºçš„é †åº
- âŒ -0.3 æ¯å€‹éŒ¯èª¤å‹•ä½œ
- âŒ -0.4 æ¯å€‹ç¼ºå¤±çš„å¿…è¦åƒæ•¸

**é æœŸçµæœ**ï¼š
- å®Œå…¨åŒ¹é…ç‡ï¼š70-85%
- å‹•ä½œ F1 åˆ†æ•¸ï¼š75-90%
- çå‹µéš¨è¨“ç·´å¢åŠ 

### ğŸš€ æ¨¡å‹éƒ¨ç½²èˆ‡æ¸¬è©¦

#### 1. éƒ¨ç½²æ¨¡å‹æœå‹™

è¨“ç·´å®Œæˆå¾Œï¼Œæ¨¡å‹ä¿å­˜åœ¨å°ˆæ¡ˆç›®éŒ„ä¸­ï¼ˆé»˜èªç‚º `outputs/` æˆ–è‡ªå®šç¾©è·¯å¾‘ï¼‰

**é¸é … Aï¼šä½¿ç”¨ FastAPIï¼ˆæ¨è–¦ï¼‰**

```bash
# å•Ÿå‹•æ¨¡å‹æœå‹™
python server.py

# æœå‹™é‹è¡Œåœ¨ http://localhost:8000
```

**é¸é … Bï¼šä½¿ç”¨ vLLMï¼ˆé«˜æ€§èƒ½ï¼‰**

```bash
pip install vllm

python -m vllm.entrypoints.openai.api_server \
    --model outputs/merged_model \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype bfloat16
```

**é¸é … Cï¼šä½¿ç”¨ Ollamaï¼ˆç°¡å–®éƒ¨ç½²ï¼‰**

```bash
# å‰µå»º Modelfile
echo 'FROM outputs/merged_model' > Modelfile.gpt-oss

# å‰µå»ºä¸¦é‹è¡Œæ¨¡å‹
ollama create gpt-oss-20b-tau -f Modelfile.gpt-oss
ollama serve
```

#### 2. é…ç½®æ¸¬è©¦ç’°å¢ƒ

ç·¨è¼¯ `configs.py`ï¼ŒæŒ‡å‘æœ¬åœ°æ¨¡å‹æœå‹™ï¼š

```python
@dataclass
class TauBenchConfig:
    # ä½¿ç”¨è¨“ç·´å¥½çš„æœ¬åœ°æ¨¡å‹
    default_model: str = "gpt-oss-20b-tau"
    default_api_key: str = "EMPTY"  # æœ¬åœ°æœå‹™ä¸éœ€è¦å¯†é‘°
    default_base_url: str = "http://localhost:8000/v1"
```

#### 3. é‹è¡Œæ¸¬è©¦

```bash
# æ¸¬è©¦è¨“ç·´å¥½çš„æ¨¡å‹
python task_tester.py \
    --tasks generated_tasks/my_tasks.json \
    --model gpt-oss-20b-tau \
    --base-url http://localhost:8000/v1 \
    --verbose \
    --save-results \
    --visualize

# æŸ¥çœ‹çµæœ
ls results/
```

### ğŸ“Š æ€§èƒ½è©•ä¼°

è¨“ç·´å®Œæˆå¾Œï¼Œå¯ä½¿ç”¨ä»¥ä¸‹æŒ‡æ¨™è©•ä¼°æ¨¡å‹ï¼š

```python
import json

# åŠ è¼‰æ¸¬è©¦çµæœ
with open('results/test_results_*.json', 'r') as f:
    results = json.load(f)

# è¨ˆç®—æˆåŠŸç‡
total = len(results['results'])
success = sum(1 for r in results['results'] if r['reward'] >= 0.99)
print(f"æˆåŠŸç‡: {success/total:.2%}")
print(f"å¹³å‡å‹•ä½œ F1: {results['summary']['avg_action_f1']:.2f}")
print(f"å¹³å‡å‹•ä½œç²¾ç¢ºç‡: {results['summary']['avg_action_precision']:.2f}")
print(f"å¹³å‡å‹•ä½œå¬å›ç‡: {results['summary']['avg_action_recall']:.2f}")
```

### ğŸ”§ è¨“ç·´æ–‡ä»¶çµæ§‹

```
APIGen-MT-5k/
â”œâ”€â”€ gpt_oss_20b_tau_bench_rl_training.ipynb  # ä¸»è¨“ç·´ notebook
â”œâ”€â”€ dataset.jsonl                             # è¨“ç·´æ•¸æ“šï¼ˆ800 ä»»å‹™ï¼‰
â”œâ”€â”€ 3.5k.jsonl                                # æ“´å±•æ•¸æ“šé›†ï¼ˆ3500+ ä»»å‹™ï¼‰
â”œâ”€â”€ server.py                                 # FastAPI æœå‹™å™¨
â”œâ”€â”€ run_training.sh                           # è¨“ç·´è…³æœ¬
â”œâ”€â”€ dgx_train.slurm                           # SLURM é…ç½®
â”œâ”€â”€ training_configs.ini                      # è¨“ç·´é…ç½®
â”œâ”€â”€ æ¨¡å‹è¨“ç·´æŒ‡å—.md                            # è¨“ç·´æŒ‡å—
â””â”€â”€ æ¨¡å‹æ¸¬è©¦æŒ‡å—.md                            # æ¸¬è©¦æŒ‡å—

# è¨“ç·´å¾Œæœƒç”Ÿæˆï¼ˆæ ¹æ“šé…ç½®ï¼‰ï¼š
outputs/
â”œâ”€â”€ checkpoint-*/                             # è¨“ç·´æª¢æŸ¥é»
â”œâ”€â”€ final_model/                              # æœ€çµ‚æ¨¡å‹ï¼ˆLoRAï¼‰
â””â”€â”€ merged_model/                             # åˆä½µå¾Œçš„å®Œæ•´æ¨¡å‹
```

### ğŸ’¡ è¨“ç·´å»ºè­°

**GPU éœ€æ±‚**ï¼š
- å»ºè­°ä½¿ç”¨ A100 æˆ– H100ï¼ˆ40GB+ VRAMï¼‰
- æœ€ä½è¦æ±‚ï¼šA40 æˆ– V100ï¼ˆ24GB VRAMï¼‰
- è¨“ç·´æ™‚é–“ï¼š4-6 å°æ™‚ï¼ˆå–æ±ºæ–¼ GPUï¼‰

**å„ªåŒ–æŠ€å·§**ï¼š
1. **å¢åŠ æ•¸æ“šé‡**ï¼šä½¿ç”¨ `3.5k.jsonl`ï¼ˆ3500+ ä»»å‹™ï¼‰ç²å¾—æ›´å¥½æ•ˆæœ
2. **èª¿æ•´ LoRA rank**ï¼šå¢åŠ åˆ° 32 æˆ– 64 æå‡æ¨¡å‹å®¹é‡
3. **èª¿æ•´å­¸ç¿’ç‡**ï¼šå˜—è©¦ 1e-5, 5e-6, 2e-5
4. **æ‰¹é‡å¤§å°**ï¼šæ ¹æ“š VRAM èª¿æ•´ `per_device_train_batch_size`

**å¸¸è¦‹å•é¡Œ**ï¼š
- **CUDA è¨˜æ†¶é«”ä¸è¶³**ï¼šæ¸›å°‘æ‰¹é‡å¤§å°æˆ–åºåˆ—é•·åº¦
- **è¨“ç·´ä¸æ”¶æ–‚**ï¼šèª¿æ•´å­¸ç¿’ç‡æˆ–å¢åŠ  warm-up steps
- **æº–ç¢ºç‡ä½ï¼ˆ<60%ï¼‰**ï¼šå¢åŠ è¨“ç·´æ•¸æ“šæˆ–èª¿æ•´çå‹µå‡½æ•¸
- **è¨“ç·´å¤ªæ…¢**ï¼šä½¿ç”¨å¤š GPU æˆ–æ¸›å°‘ epoch

æ›´å¤šè©³ç´°ä¿¡æ¯è«‹åƒè€ƒï¼š
- ğŸ“– [æ¨¡å‹è¨“ç·´æŒ‡å—.md](æ¨¡å‹è¨“ç·´æŒ‡å—.md) - å®Œæ•´è¨“ç·´æ•™ç¨‹
- ğŸ“– [æ¨¡å‹æ¸¬è©¦æŒ‡å—.md](æ¨¡å‹æ¸¬è©¦æŒ‡å—.md) - éƒ¨ç½²å’Œæ¸¬è©¦æŒ‡å—
- ğŸ“Š [SUMMARY.md](SUMMARY.md) - å°ˆæ¡ˆæ‘˜è¦å’Œæ¦‚è¦½

## æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. API é€£æ¥å¤±æ•—

**å•é¡Œ**: `Connection refused` æˆ– `API key invalid`

**è§£æ±ºæ–¹æ¡ˆ**:
- æª¢æŸ¥ `configs.py` ä¸­çš„ API å¯†é‘°å’Œ URL
- é©—è­‰ç¶²çµ¡é€£æ¥
- ç¢ºèª API æœå‹™æ­£å¸¸é‹è¡Œ

```bash
# æ¸¬è©¦ API é€£æ¥
curl -H "Authorization: Bearer YOUR_API_KEY" \
     https://api.openai.com/v1/models
```

#### 2. å·¥å…·åŸ·è¡Œå¤±æ•—

**å•é¡Œ**: `Tool 'xxx' not found` æˆ– `Tool execution failed`

**è§£æ±ºæ–¹æ¡ˆ**:
- ç¢ºèª `envs/retail/` ç›®éŒ„å­˜åœ¨
- æª¢æŸ¥ `tools/` ç›®éŒ„ä¸­çš„å·¥å…·å¯¦ç¾
- é©—è­‰æ•¸æ“šæ–‡ä»¶ï¼ˆusers.json, orders.json, products.jsonï¼‰æ ¼å¼æ­£ç¢º

```bash
# é©—è­‰æ•¸æ“šæ–‡ä»¶
python -c "import json; json.load(open('envs/retail/users.json'))"
```

#### 3. ä»»å‹™é©—è­‰å¤±æ•—

**å•é¡Œ**: `Validation failed` æˆ– `User not found`

**è§£æ±ºæ–¹æ¡ˆ**:
- æª¢æŸ¥ä»»å‹™ä¸­çš„ç”¨æˆ¶IDå’Œè¨‚å–®IDæ˜¯å¦å­˜åœ¨æ–¼æ•¸æ“šæ–‡ä»¶ä¸­
- ä½¿ç”¨ `--validate-only` é¸é …å–®ç¨é‹è¡Œé©—è­‰
- æŸ¥çœ‹è©³ç´°çš„é©—è­‰å ±å‘Š

```bash
# åƒ…é©—è­‰ä»»å‹™
python task_tester.py --tasks my_tasks.json --validate-only --verbose
```

#### 4. å†…å­˜ä¸è¶³

**å•é¡Œ**: `MemoryError` æˆ–ç³»çµ±è®Šæ…¢

**è§£æ±ºæ–¹æ¡ˆ**:
- æ¸›å°‘ `--threads` åƒæ•¸å€¼
- åˆ†æ‰¹è™•ç†ä»»å‹™
- å¢åŠ ç³»çµ±å…§å­˜æˆ–ä½¿ç”¨è™›æ“¬å…§å­˜

```bash
# åˆ†æ‰¹æ¸¬è©¦
python task_tester.py --tasks batch1.json --threads 2
python task_tester.py --tasks batch2.json --threads 2
```

#### 5. AgentFlow ç”Ÿæˆæ™‚é–“éé•·

**å•é¡Œ**: AgentFlow ç”Ÿæˆä¸€å€‹ä»»å‹™éœ€è¦å¾ˆé•·æ™‚é–“

**è§£æ±ºæ–¹æ¡ˆ**:
- æ¸›å°‘ `--agentflow-turns` åƒæ•¸
- å°ç°¡å–®ä»»å‹™ä½¿ç”¨ç›´æ¥æ¨¡å¼ï¼ˆä¸åŠ  `--agentflow`ï¼‰
- æª¢æŸ¥ API éŸ¿æ‡‰æ™‚é–“

```bash
# ä½¿ç”¨è¼ƒå°‘çš„è¼ªæ¬¡
python task_pipeline.py --num-tasks 10 --agentflow --agentflow-turns 3
```

### èª¿è©¦æŠ€å·§

#### å•Ÿç”¨è©³ç´°æ—¥èªŒ

```bash
# è¨­ç½®æ—¥èªŒç´šåˆ¥ç‚º DEBUG
export LOG_LEVEL=DEBUG

# é‹è¡Œæ™‚å•Ÿç”¨è©³ç´°è¼¸å‡º
python task_tester.py --tasks my_tasks.json --verbose
```

#### ä½¿ç”¨æ¨¡æ“¬æ¨¡å¼

```bash
# ä¸èª¿ç”¨ LLMï¼Œå¿«é€Ÿæ¸¬è©¦æµç¨‹
python task_tester.py --tasks my_tasks.json --dry-run --verbose
```

#### æª¢æŸ¥ä»»å‹™æ ¼å¼

```python
# ä½¿ç”¨ Python é©—è­‰ä»»å‹™æ ¼å¼
import json

with open('generated_tasks/my_tasks.json', 'r') as f:
    tasks = json.load(f)
    for i, task in enumerate(tasks):
        print(f"Task {i}:")
        print(f"  Query: {task['task']['q'][:50]}...")
        print(f"  Actions: {len(task['task']['agt'])}")
        print(f"  Valid: {task.get('validation_report', {}).get('valid', 'unknown')}")
```

### ç²å–å¹«åŠ©

å¦‚é‡åˆ°å•é¡Œï¼š

1. **æŸ¥çœ‹æ—¥èªŒæ–‡ä»¶**: `task_tester.log`, `task_generator.log`
2. **å•Ÿç”¨è©³ç´°æ¨¡å¼**: ä½¿ç”¨ `--verbose` åƒæ•¸
3. **æª¢æŸ¥æ–‡æª”**: é–±è®€ `AGENTFLOW_README.md` å’Œ `Compare_agentflow_direct.md`
4. **é‹è¡Œæ¸¬è©¦**: ä½¿ç”¨ `--dry-run` æˆ– `--validate-only` é€²è¡Œå¿«é€Ÿæ¸¬è©¦

---

# English Documentation

## Project Overview

**APIGen-MT-5k** is an advanced multi-turn task generation and testing system designed for customer service scenarios. The system implements the **AgentFlow** multi-turn iterative architecture, supports dual-model collaboration (GPT-4o + GPT-OSS-120B), and provides a complete pipeline for task generation, validation, testing, and analysis.

### ğŸ¯ Key Features

- **ğŸ”„ AgentFlow Architecture**: Multi-turn iterative generation with Planner, Executor, Verifier, and Generator collaboration
- **ğŸ¤– Dual-Model System**: GPT-4o (user model) + GPT-OSS-120B (assistant model) collaboration
- **âœ… Intelligent Validation**: Automatic validation of data consistency, user ID consistency, and tool invocation correctness
- **ğŸ“Š Comprehensive Testing**: Parallel testing, multiple evaluation metrics, and result visualization
- **ğŸ”§ Flexible Configuration**: Centralized configuration management via `configs.py`
- **ğŸ“ˆ Detailed Reports**: JSON, CSV, and visualization reports for in-depth analysis
- **ğŸ“ Model Training**: Support for GPT-OSS 20B model reinforcement learning training (GRPO + SFT) for custom customer service task models

### ğŸ—ï¸ System Architecture

*(Same architecture diagram as Chinese version)*

## Directory Structure

*(Same directory structure as Chinese version)*

## Quick Start

### 1. Install Dependencies

```bash
# Required dependencies
pip install openai

# Optional dependencies (for progress bars and visualizations)
pip install tqdm pandas matplotlib
```

### 2. Configure API Keys

Edit the `configs.py` file and set your API keys:

```python
@dataclass
class TauBenchConfig:
    # GPT-4o (user model)
    user_model: str = "gpt-4o"
    user_api_key: str = "your-gpt4o-api-key"
    user_base_url: str = "https://api.openai.com/v1"
    
    # GPT-OSS-120B (assistant model)
    default_model: str = "vllm-a40-gpt-oss-120b"
    default_api_key: str = "your-gpt-oss-api-key"
    default_base_url: str = "https://your-api-endpoint/api"
```

### 3. Basic Usage

```bash
# Generate tasks (AgentFlow mode)
python task_pipeline.py --num-tasks 10 --agentflow --output generated_tasks/my_tasks.json

# Test tasks (dual-model)
python task_tester.py --tasks generated_tasks/my_tasks.json --dual-model --verbose --save-results --visualize

# View results
ls results/
```

## Core Components

### 1. Task Tester (`task_tester.py`)

Task tester with dual-model support and multiple testing modes.

#### Main Features

- âœ… **Dual-Model Testing**: GPT-4o enhances queries + GPT-OSS-120B executes tasks
- âœ… **Parallel Testing**: Multi-threaded parallel processing for efficiency
- âœ… **Multiple Modes**: Real execution, simulated execution, validation-only
- âœ… **Detailed Metrics**: Precision, recall, F1 score, output match rate, etc.
- âœ… **Result Visualization**: Automatic chart and statistical report generation

#### Command Line Arguments

**Input Options**:
```bash
--tasks FILE                    # JSON file containing tasks to test
```

**Dual-Model Options**:
```bash
--dual-model                    # Enable dual-model approach
--enhance-query                 # Enhance query using user model
```

**Assistant Model Configuration** (GPT-OSS-120B):
```bash
--model MODEL                   # Model name
--api-key KEY                   # API key
--base-url URL                  # API base URL
```

**User Model Configuration** (GPT-4o):
```bash
--user-model MODEL              # User model name
--user-api-key KEY              # User model API key
--user-base-url URL             # User model API base URL
```

**Tool Configuration**:
```bash
--envs-path PATH                # Retail environment path (default: envs/retail)
```

**Execution Options**:
```bash
--threads N                     # Number of parallel threads (default: 1)
--verbose                       # Verbose output
--dry-run                       # Dry run (no LLM calls)
--validate-only                 # Validate tasks only (no model calls)
```

**Output Options**:
```bash
--output-dir DIR                # Results directory (default: results)
--save-results                  # Save detailed results
--visualize                     # Generate visualizations
```

#### Usage Examples

```bash
# Basic testing
python task_tester.py --tasks generated_tasks/my_tasks.json --verbose

# Dual-model testing with visualization
python task_tester.py \
    --tasks generated_tasks/my_tasks.json \
    --dual-model \
    --enhance-query \
    --threads 4 \
    --save-results \
    --visualize

# Validation only (no model calls)
python task_tester.py --tasks generated_tasks/my_tasks.json --validate-only

# Dry run (quick test)
python task_tester.py --tasks generated_tasks/my_tasks.json --dry-run --verbose
```

### 2. Task Generator (`task_generator.py`)

Task generator supporting both AgentFlow multi-turn generation and direct generation modes.

#### AgentFlow Architecture

AgentFlow is a multi-turn iterative generation architecture with the following components:

*(Same AgentFlow architecture diagram and component descriptions as Chinese version)*

#### Usage Examples

```python
from task_generator import TauBenchOpenAIGenerator

# Create generator
generator = TauBenchOpenAIGenerator("envs/retail")

# Generate single task with AgentFlow
result = generator.generate_task_with_agentflow(
    max_turns=5,
    include_metadata=True
)

# Batch generate tasks
tasks = generator.generate_diverse_tasks(
    num_tasks=10,
    use_agentflow=True
)

# Save tasks
generator.save_tasks_to_file(tasks, "generated_tasks/my_tasks.json")
```

For detailed AgentFlow documentation, see [AGENTFLOW_README.md](AGENTFLOW_README.md)

### 3. Task Pipeline (`task_pipeline.py`)

Task generation pipeline integrating generation, validation, review, and refinement.

#### Main Features

- ğŸ”„ **Iterative Refinement**: Automatic iterations until tasks pass validation
- ğŸ‘¥ **Review Committee**: Multiple reviewers evaluate task quality
- ğŸ” **User ID Validation**: Ensures consistent user IDs across all actions
- ğŸ“Š **Statistical Reports**: Detailed generation statistics and failure analysis
- ğŸ¯ **Scenario Diversity**: Ensures generated tasks cover multiple scenario types

#### Command Line Arguments

```bash
--num-tasks N                   # Number of tasks to generate (default: 3)
--max-iterations N              # Max iterations per task (default: 3)
--output FILE                   # Output file path (default: generated_tasks/Sampled_Tasks.json)
--no-user-id-validation         # Disable user ID consistency validation
--committee-size N              # Review committee size (default: 3)
--agentflow                     # Use AgentFlow multi-turn generation
--agentflow-turns N             # Max turns for AgentFlow (default: 5)
```

#### Usage Examples

```bash
# Basic generation (direct mode)
python task_pipeline.py --num-tasks 10 --output generated_tasks/my_tasks.json

# Generate with AgentFlow
python task_pipeline.py --num-tasks 10 --agentflow --agentflow-turns 5 --output generated_tasks/agentflow_tasks.json

# Increase iterations and committee size
python task_pipeline.py --num-tasks 5 --max-iterations 5 --committee-size 5 --output generated_tasks/high_quality_tasks.json

# Disable user ID validation (use only in special cases)
python task_pipeline.py --num-tasks 10 --no-user-id-validation --output generated_tasks/tasks.json
```

#### Pipeline Flow

*(Same pipeline flow diagram as Chinese version)*

### 4. Configuration System (`configs.py`)

Centralized configuration management system.

#### Main Configuration Options

*(Same configuration options as Chinese version)*

## Usage Examples

### Complete Workflow

```bash
# Step 1: Generate high-quality tasks with AgentFlow
python task_pipeline.py \
    --num-tasks 20 \
    --agentflow \
    --agentflow-turns 5 \
    --max-iterations 3 \
    --committee-size 3 \
    --output generated_tasks/production_tasks.json

# Step 2: Test tasks with dual-model
python task_tester.py \
    --tasks generated_tasks/production_tasks.json \
    --dual-model \
    --enhance-query \
    --threads 8 \
    --save-results \
    --visualize \
    --output-dir results/production_test

# Step 3: View results
ls results/production_test/
# test_results_*.json      - Detailed results
# test_summary_*.json      - Summary report
# test_summary_*.csv       - CSV format
# test_visualization_*.png - Visualization charts
```

### Compare AgentFlow vs Direct Mode

```bash
# Use script to compare both methods
python scripts/compare_blueprint_agentflow.py \
    --num-tasks 10 \
    --direct-out generated_tasks/test_direct.json \
    --agent-out generated_tasks/test_agentflow.json \
    --agentflow-turns 5
```

For detailed comparison analysis, see [Compare_agentflow_direct.md](Compare_agentflow_direct.md)

### Analyze Data References

```bash
# Analyze real data references in tasks
python scripts/analyze_real_data_references.py \
    generated_tasks/my_tasks.json \
    --envs-path envs/retail \
    --output analysis_report.json
```

## Configuration System

### Modifying Configuration

You can modify configuration in the following ways:

1. **Directly edit `configs.py`** (recommended for permanent changes)
2. **Command-line argument overrides** (for temporary changes)
3. **Environment variables** (for sensitive information)

### Configuration Priority

```
Command-line arguments > configs.py settings > Default values
```

### Example: Custom Configuration

```python
# Use custom configuration in code
from configs import TauBenchConfig

config = TauBenchConfig()
config.num_tasks = 50
config.temperature = 0.7
config.max_workers = 4

# Use custom configuration
from task_pipeline import TaskConfigurationPipeline, PipelineConfig

pipeline_config = PipelineConfig(
    envs_path=config.envs_path,
    max_iterations=5,
    committee_size=5,
    use_agentflow=True
)

pipeline = TaskConfigurationPipeline(pipeline_config)
```

## Output Description

*(Same output descriptions as Chinese version)*

## Model Training & Testing

This project supports **GPT-OSS 20B** model training via reinforcement learning for customer service task execution. The training follows a two-phase approach:

### ğŸ“š Complete Documentation

- **[æ¨¡å‹è¨“ç·´æŒ‡å—.md](æ¨¡å‹è¨“ç·´æŒ‡å—.md)** - Detailed model training tutorial (Chinese)
- **[æ¨¡å‹æ¸¬è©¦æŒ‡å—.md](æ¨¡å‹æ¸¬è©¦æŒ‡å—.md)** - Model deployment and testing guide (Chinese)

### ğŸ› ï¸ Training Environment Setup

Multiple training environment options are available. Choose the one that fits your setup:

#### Local Environment (Recommended for Development)

```bash
# Standard installation
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes

# For A100/H100 GPUs (better performance)
pip install "unsloth[cu121-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

#### NVIDIA DGX Spark (For Large-Scale Training)

Train models up to 200B parameters (e.g., gpt-oss-120b):

```bash
# 1. Download and build Docker image
wget -O Dockerfile "https://raw.githubusercontent.com/unslothai/notebooks/main/Dockerfile_DGX_Spark"
docker build -f Dockerfile -t unsloth-dgx-spark .

# 2. Launch container
docker run -it --gpus=all --net=host --ipc=host \
    -v $(pwd):$(pwd) -w $(pwd) unsloth-dgx-spark

# 3. Train inside container
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root
```

**Memory Requirements**:
- GPT-OSS 20B: 24-40 GB VRAM (4-bit quantization)
- GPT-OSS 120B: ~68 GB unified memory (QLoRA 4-bit)

**Resources**:
- ğŸ“š [Unsloth DGX Spark Documentation](https://unsloth.ai/docs/basics/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth)
- ğŸ³ [DGX Spark Dockerfile](https://raw.githubusercontent.com/unslothai/notebooks/main/Dockerfile_DGX_Spark)

#### Cloud Platforms

**Google Colab** (Free T4 GPU):
```python
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
```

**AWS/Azure/GCP** (A100/H100):
```bash
pip install "unsloth[cu121-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

**See [æ¨¡å‹è¨“ç·´æŒ‡å—.md](æ¨¡å‹è¨“ç·´æŒ‡å—.md) for detailed setup instructions**

### ğŸ“ Training Pipeline Overview

#### Phase 1: SFT Pretraining (30-60 minutes)

Supervised fine-tuning to teach the model JSON format and basic tool selection:

```bash
# In project root directory
jupyter notebook gpt_oss_20b_tau_bench_rl_training.ipynb

# Execute Cell 1-16 (SFT Training)
```

**Features**:
- Uses 800 labeled examples
- Learns correct tool calling format
- JSON valid rate: ~95%
- Action accuracy: ~90%

#### Phase 2: GRPO Training (2-4 hours)

Reinforcement learning for policy optimization:

```bash
# Continue executing Cell 17-21 (GRPO Training)
```

**Reward Mechanism**:
- âœ… +1.0 per correct action name
- âœ… +0.5 per correct argument set
- âœ… +0.5 for correct ordering
- âŒ -0.3 per incorrect action
- âŒ -0.4 per missing required argument

**Expected Results**:
- Exact match rate: 70-85%
- Action F1 score: 75-90%
- Rewards increase over training

### ğŸš€ Model Deployment & Testing

#### 1. Deploy Model Service

After training, the model is saved in the project directory (default `outputs/` or custom path)

**Option A: Use FastAPI (Recommended)**

```bash
# Start model service
python server.py

# Service runs on http://localhost:8000
```

**Option B: Use vLLM (High Performance)**

```bash
pip install vllm

python -m vllm.entrypoints.openai.api_server \
    --model outputs/merged_model \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype bfloat16
```

**Option C: Use Ollama (Simple Deployment)**

```bash
# Create Modelfile
echo 'FROM outputs/merged_model' > Modelfile.gpt-oss

# Create and run model
ollama create gpt-oss-20b-tau -f Modelfile.gpt-oss
ollama serve
```

#### 2. Configure Test Environment

Edit `configs.py` to point to local model service:

```python
@dataclass
class TauBenchConfig:
    # Use trained local model
    default_model: str = "gpt-oss-20b-tau"
    default_api_key: str = "EMPTY"  # Local service doesn't need key
    default_base_url: str = "http://localhost:8000/v1"
```

#### 3. Run Tests

```bash
# Test trained model
python task_tester.py \
    --tasks generated_tasks/my_tasks.json \
    --model gpt-oss-20b-tau \
    --base-url http://localhost:8000/v1 \
    --verbose \
    --save-results \
    --visualize

# View results
ls results/
```

### ğŸ“Š Performance Evaluation

After training, evaluate the model using these metrics:

```python
import json

# Load test results
with open('results/test_results_*.json', 'r') as f:
    results = json.load(f)

# Calculate success rate
total = len(results['results'])
success = sum(1 for r in results['results'] if r['reward'] >= 0.99)
print(f"Success Rate: {success/total:.2%}")
print(f"Avg Action F1: {results['summary']['avg_action_f1']:.2f}")
print(f"Avg Action Precision: {results['summary']['avg_action_precision']:.2f}")
print(f"Avg Action Recall: {results['summary']['avg_action_recall']:.2f}")
```

### ğŸ”§ Training File Structure

```
APIGen-MT-5k/
â”œâ”€â”€ gpt_oss_20b_tau_bench_rl_training.ipynb  # Main training notebook
â”œâ”€â”€ dataset.jsonl                             # Training data (800 tasks)
â”œâ”€â”€ 3.5k.jsonl                                # Extended dataset (3500+ tasks)
â”œâ”€â”€ server.py                                 # FastAPI server
â”œâ”€â”€ run_training.sh                           # Training script
â”œâ”€â”€ dgx_train.slurm                           # SLURM configuration
â”œâ”€â”€ training_configs.ini                      # Training configs
â”œâ”€â”€ æ¨¡å‹è¨“ç·´æŒ‡å—.md                            # Training guide
â””â”€â”€ æ¨¡å‹æ¸¬è©¦æŒ‡å—.md                            # Testing guide

# After training (based on configuration):
outputs/
â”œâ”€â”€ checkpoint-*/                             # Training checkpoints
â”œâ”€â”€ final_model/                              # Final model (LoRA)
â””â”€â”€ merged_model/                             # Merged complete model
```

### ğŸ’¡ Training Recommendations

**GPU Requirements**:
- Recommended: A100 or H100 (40GB+ VRAM)
- Minimum: A40 or V100 (24GB VRAM)
- Training time: 4-6 hours (depending on GPU)

**Optimization Tips**:
1. **Increase data**: Use `3.5k.jsonl` (3500+ tasks) for better results
2. **Adjust LoRA rank**: Increase to 32 or 64 for more capacity
3. **Tune learning rate**: Try 1e-5, 5e-6, 2e-5
4. **Batch size**: Adjust `per_device_train_batch_size` based on VRAM

**Common Issues**:
- **CUDA out of memory**: Reduce batch size or sequence length
- **Training not converging**: Adjust learning rate or increase warm-up steps
- **Low accuracy (<60%)**: Increase training data or adjust reward function
- **Training too slow**: Use multi-GPU or reduce epochs

For more details, see:
- ğŸ“– [æ¨¡å‹è¨“ç·´æŒ‡å—.md](æ¨¡å‹è¨“ç·´æŒ‡å—.md) - Complete training tutorial
- ğŸ“– [æ¨¡å‹æ¸¬è©¦æŒ‡å—.md](æ¨¡å‹æ¸¬è©¦æŒ‡å—.md) - Deployment and testing guide
- ğŸ“Š [SUMMARY.md](SUMMARY.md) - Project overview and summary

## Troubleshooting

### Common Issues

#### 1. API Connection Failure

**Problem**: `Connection refused` or `API key invalid`

**Solution**:
- Check API keys and URLs in `configs.py`
- Verify network connection
- Confirm API service is running

```bash
# Test API connection
curl -H "Authorization: Bearer YOUR_API_KEY" \
     https://api.openai.com/v1/models
```

#### 2. Tool Execution Failure

**Problem**: `Tool 'xxx' not found` or `Tool execution failed`

**Solution**:
- Confirm `envs/retail/` directory exists
- Check tool implementations in `tools/` directory
- Verify data file formats (users.json, orders.json, products.json)

```bash
# Verify data files
python -c "import json; json.load(open('envs/retail/users.json'))"
```

#### 3. Task Validation Failure

**Problem**: `Validation failed` or `User not found`

**Solution**:
- Check if user IDs and order IDs in tasks exist in data files
- Run validation separately with `--validate-only`
- Review detailed validation reports

```bash
# Validate tasks only
python task_tester.py --tasks my_tasks.json --validate-only --verbose
```

#### 4. Out of Memory

**Problem**: `MemoryError` or system slowdown

**Solution**:
- Reduce `--threads` parameter value
- Process tasks in batches
- Increase system memory or use virtual memory

```bash
# Test in batches
python task_tester.py --tasks batch1.json --threads 2
python task_tester.py --tasks batch2.json --threads 2
```

#### 5. AgentFlow Generation Takes Too Long

**Problem**: AgentFlow takes a long time to generate a single task

**Solution**:
- Reduce `--agentflow-turns` parameter
- Use direct mode (without `--agentflow`) for simple tasks
- Check API response time

```bash
# Use fewer turns
python task_pipeline.py --num-tasks 10 --agentflow --agentflow-turns 3
```

### Debugging Tips

#### Enable Verbose Logging

```bash
# Set log level to DEBUG
export LOG_LEVEL=DEBUG

# Enable verbose output at runtime
python task_tester.py --tasks my_tasks.json --verbose
```

#### Use Simulation Mode

```bash
# No LLM calls, use simulated responses
python task_tester.py --tasks my_tasks.json --dry-run --verbose
```

#### Check Task Format

```python
# Verify task format using Python
import json

with open('generated_tasks/my_tasks.json', 'r') as f:
    tasks = json.load(f)
    for i, task in enumerate(tasks):
        print(f"Task {i}:")
        print(f"  Query: {task['task']['q'][:50]}...")
        print(f"  Actions: {len(task['task']['agt'])}")
        print(f"  Valid: {task.get('validation_report', {}).get('valid', 'unknown')}")
```

### Getting Help

If you encounter issues:

1. **Check log files**: `task_tester.log`, `task_generator.log`
2. **Enable verbose mode**: Use `--verbose` parameter
3. **Read documentation**: 
   - Review `AGENTFLOW_README.md` and `Compare_agentflow_direct.md`
   - For model training: See [æ¨¡å‹è¨“ç·´æŒ‡å—.md](æ¨¡å‹è¨“ç·´æŒ‡å—.md)
   - For model testing: See [æ¨¡å‹æ¸¬è©¦æŒ‡å—.md](æ¨¡å‹æ¸¬è©¦æŒ‡å—.md)
4. **Run tests**: Use `--dry-run` or `--validate-only` for quick tests

---

## ğŸ“š Additional Documentation

### Core System
- **[AGENTFLOW_README.md](AGENTFLOW_README.md)** - Detailed AgentFlow architecture documentation
- **[Compare_agentflow_direct.md](Compare_agentflow_direct.md)** - Comparison of generation methods
- **[SUMMARY.md](SUMMARY.md)** - Project overview and summary

### Model Training & Testing
- **[æ¨¡å‹è¨“ç·´æŒ‡å—.md](æ¨¡å‹è¨“ç·´æŒ‡å—.md)** - Complete GPT-OSS 20B training guide (Chinese)
- **[æ¨¡å‹æ¸¬è©¦æŒ‡å—.md](æ¨¡å‹æ¸¬è©¦æŒ‡å—.md)** - Model deployment and testing guide (Chinese)
- **[GPT_OSS_120B_Task_Performance_Report.md](GPT_OSS_120B_Task_Performance_Report.md)** - Performance analysis report

### Analysis Scripts
- **scripts/analysis_helpers.py** - Helper functions for analysis
- **scripts/analyze_generated_tasks.py** - Analyze generated task statistics
- **scripts/analyze_real_data_references.py** - Analyze real data references
- **scripts/compare_blueprint_agentflow.py** - Compare generation methods

---

## ğŸš€ Quick Links

| Task | Documentation | Script/File |
|------|--------------|-------------|
| Generate tasks | [Quick Start](#quick-start) | `task_pipeline.py` |
| Test tasks | [Task Tester](#1-task-tester-task_testerpy) | `task_tester.py` |
| Train model | [æ¨¡å‹è¨“ç·´æŒ‡å—.md](æ¨¡å‹è¨“ç·´æŒ‡å—.md) | `gpt_oss_20b_tau_bench_rl_training.ipynb` |
| Deploy model | [æ¨¡å‹æ¸¬è©¦æŒ‡å—.md](æ¨¡å‹æ¸¬è©¦æŒ‡å—.md) | `server.py` |
| Analyze results | [Analysis Scripts](#analysis-scripts) | `scripts/analyze_*.py` |
| Configure system | [Configuration System](#configuration-system) | `configs.py` |

---

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions and support, please open an issue in the repository.

---

**Last Updated**: 2026-01-28  
**Version**: 2.0.0  
**New in 2.0**: Model training support with GPT-OSS 20B (GRPO + SFT)
