# GPT-OSS 20B æ¨¡å‹åœ¨ Tau-Bench ä¸Šçš„æ¸¬è©¦æŒ‡å—

æœ¬æŒ‡å—èªªæ˜å¦‚ä½•ä½¿ç”¨ `gpt_oss_20b_tau_bench_rl_training.ipynb` è¨“ç·´å‡ºçš„æ¨¡å‹åœ¨ tau-bench ä¸Šé€²è¡Œæ¸¬è©¦ã€‚

## ğŸ“‹ ç›®éŒ„
1. [è¨“ç·´æ¨¡å‹ä¿å­˜ä½ç½®](#è¨“ç·´æ¨¡å‹ä¿å­˜ä½ç½®)
2. [éƒ¨ç½²æ¨¡å‹æœå‹™](#éƒ¨ç½²æ¨¡å‹æœå‹™)
3. [é…ç½® Tau-Bench](#é…ç½®-tau-bench)
4. [é‹è¡Œæ¸¬è©¦](#é‹è¡Œæ¸¬è©¦)
5. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## 1ï¸âƒ£ è¨“ç·´æ¨¡å‹ä¿å­˜ä½ç½®

åŸ·è¡Œ `gpt_oss_20b_tau_bench_rl_training.ipynb` å¾Œï¼Œæ¨¡å‹æœƒä¿å­˜åˆ°ä»¥ä¸‹ä½ç½®ï¼š

- **LoRA é©é…å™¨**: `outputs/tau_bench_rl/final_model/`
  - åŒ…å« LoRA æ¬Šé‡å’Œé…ç½®
  - éœ€è¦åŸå§‹ GPT-OSS 20B åŸºç¤æ¨¡å‹æ‰èƒ½ä½¿ç”¨
  
- **åˆä½µå¾Œçš„å®Œæ•´æ¨¡å‹**: `outputs/tau_bench_rl/merged_model/`
  - å°‡ LoRA æ¬Šé‡åˆä½µåˆ°åŸºç¤æ¨¡å‹ä¸­
  - å¯ç¨ç«‹ä½¿ç”¨ï¼Œæ¨è–¦ç”¨æ–¼æ¸¬è©¦
  - æ–‡ä»¶åŒ…æ‹¬ï¼š
    - `model.safetensors` æˆ– `pytorch_model.bin`
    - `config.json`
    - `tokenizer.json`, `tokenizer_config.json`
    - `special_tokens_map.json`

**æ¨è–¦ä½¿ç”¨**: `outputs/tau_bench_rl/merged_model/` é€²è¡Œæ¸¬è©¦ï¼Œå› ç‚ºå®ƒæ˜¯å®Œæ•´çš„æ¨¡å‹ï¼Œä¸éœ€è¦é¡å¤–çš„åŸºç¤æ¨¡å‹ã€‚

---

## 2ï¸âƒ£ éƒ¨ç½²æ¨¡å‹æœå‹™

Tau-bench ä½¿ç”¨ OpenAI å…¼å®¹çš„ API ä¾†èª¿ç”¨æ¨¡å‹ï¼Œæ‰€ä»¥éœ€è¦å°‡è¨“ç·´å¥½çš„æ¨¡å‹éƒ¨ç½²ç‚º API æœå‹™ã€‚

### é¸é … A: ä½¿ç”¨ fastapi (FastAPI) + uvicornï¼ˆæ¨è–¦ç”¨æ–¼ç°¡å–®éƒ¨ç½²ï¼‰

#### å®‰è£ä¾è³´
```bash
pip install fastapi uvicorn pydantic
```
#### ä½¿ç”¨ fastapi + uvicorn å•Ÿå‹•
```bash
python server.py
```

API ç«¯é»ç‚ºï¼š`http://localhost:8080`


### é¸é … B: ä½¿ç”¨ Ollamaï¼ˆæ¨è–¦ç”¨æ–¼ç°¡å–®éƒ¨ç½²ï¼‰

Ollama æä¾›ç°¡å–®çš„æœ¬åœ°æ¨¡å‹æœå‹™ã€‚

#### å®‰è£ Ollama
```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# Windows: å¾ https://ollama.com/download ä¸‹è¼‰å®‰è£åŒ…
```

#### å‰µå»º Modelfile
å‰µå»º `Modelfile.gpt-oss-20b-tau`:
```
FROM outputs/tau_bench_rl/merged_model

PARAMETER temperature 0
PARAMETER top_p 0.9
PARAMETER stop "<|im_end|>"
PARAMETER stop "</s>"
```

#### å‰µå»ºä¸¦é‹è¡Œæ¨¡å‹
```bash
ollama create gpt-oss-20b-tau -f Modelfile.gpt-oss-20b-tau
ollama serve
```

Ollama é»˜èªåœ¨ `http://localhost:11434` æä¾› OpenAI å…¼å®¹çš„ APIã€‚

### é¸é … C: ä½¿ç”¨ vLLMï¼ˆæ¨è–¦ç”¨æ–¼é«˜æ€§èƒ½ï¼‰

vLLM æ˜¯é«˜æ€§èƒ½çš„æ¨ç†æœå‹™å™¨ï¼Œé©åˆå¤§è¦æ¨¡æ¸¬è©¦ã€‚

#### å®‰è£ vLLM
```bash
pip install vllm
```

#### å•Ÿå‹•æœå‹™
```bash
python -m vllm.entrypoints.openai.api_server \
    --model outputs/tau_bench_rl/merged_model \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1 \
    --dtype bfloat16 \
    --max-model-len 2048 \
    --enforce-eager
```

åƒæ•¸èªªæ˜ï¼š
- `--model`: æ¨¡å‹è·¯å¾‘ï¼ˆä½¿ç”¨ merged_modelï¼‰
- `--port`: API æœå‹™ç«¯å£
- `--tensor-parallel-size`: GPU æ•¸é‡ï¼ˆå–®å¡è¨­ç‚º 1ï¼‰
- `--dtype`: æ•¸æ“šé¡å‹ï¼ˆbfloat16 æˆ– float16ï¼‰
- `--max-model-len`: æœ€å¤§åºåˆ—é•·åº¦ï¼ˆèˆ‡è¨“ç·´æ™‚çš„ max_seq_length ä¸€è‡´ï¼‰

æœå‹™å•Ÿå‹•å¾Œï¼ŒAPI ç«¯é»ç‚ºï¼š`http://localhost:8000/v1`

#### æ¸¬è©¦æœå‹™æ˜¯å¦æ­£å¸¸
```bash
curl http://localhost:8000/v1/models
```

---

## 3ï¸âƒ£ é…ç½® Tau-Bench

### æ–¹æ³•: ä¿®æ”¹ tool_calling_agent.pyï¼ˆæ¨è–¦ï¼‰

ç·¨è¼¯ `tau_bench/agents/tool_calling_agent.py`ï¼Œä¿®æ”¹ OpenAI å®¢æˆ¶ç«¯é…ç½®ï¼š

```python
# åŸå§‹é…ç½®ï¼ˆç´„åœ¨ç¬¬ 7-11 è¡Œï¼‰
api_key = "YOUR_API_KEY"
client = OpenAI(
    api_key=api_key,
    base_url="",
)

# ä¿®æ”¹ç‚ºæœ¬åœ°æœå‹™
# å¦‚æœä½¿ç”¨ vLLM:
api_key = "EMPTY"  # vLLM ä¸éœ€è¦çœŸå¯¦çš„ API key
client = OpenAI(
    api_key=api_key,
    base_url="http://localhost:8000/v1",  # vLLM ç«¯é»
)

# å¦‚æœä½¿ç”¨ Ollama:
api_key = "ollama"
client = OpenAI(
    api_key=api_key,
    base_url="http://localhost:11434/v1",  # Ollama ç«¯é»
)

# å¦‚æœä½¿ç”¨ FastAPI + uvicorn:
api_key = "EMPTY"
client = OpenAI(
    api_key=api_key,
    base_url="http://localhost:8000/v1",  # FastAPI ç«¯é»
)
```

---

## 4ï¸âƒ£ é‹è¡Œæ¸¬è©¦

### ä½¿ç”¨åŸå§‹ run.py

```bash
# æ¸¬è©¦ retail ç’°å¢ƒçš„å‰ 1 å€‹ä»»å‹™
python run.py \
    --model gpt-oss-20b-tau \
    --model-provider openai \
    --env retail \
    --agent-strategy tool-calling \
    --temperature 0.0 \
    --user-model vllm-a40-gpt-oss-120b \
    --user-model-provider openai \
    --user-strategy llm \
    --task-split test \
    --start-index 0 \
    --end-index 1 \
    --log-dir results/custom-model \
    --max-concurrency 1

# å®Œæ•´æ¸¬è©¦ï¼ˆæ‰€æœ‰ä»»å‹™ï¼‰
python run.py \
    --model gpt-oss-20b-tau \
    --model-provider openai \
    --env retail \
    --agent-strategy tool-calling \
    --temperature 0.0 \
    --task-split test \
    --start-index 0 \
    --end-index -1 \
    --log-dir results/custom-model \
    --max-concurrency 1
```

### æ¸¬è©¦ç‰¹å®šä»»å‹™

```bash
python run.py \
    --model gpt-oss-20b-tau \
    --model-provider openai \
    --env retail \
    --task-ids 0 5 10 15 20 \
    --log-dir results/custom-model
```

---

## 5ï¸âƒ£ æŸ¥çœ‹çµæœ

æ¸¬è©¦å®Œæˆå¾Œï¼Œçµæœæœƒä¿å­˜åœ¨ `results/` ç›®éŒ„ä¸‹ï¼Œæ–‡ä»¶åæ ¼å¼ç‚ºï¼š

```
{agent-strategy}-{model-name}-{temperature}_range_{start}-{end}_user-{user-model}-{user-strategy}_{timestamp}.json
```

### çµæœåˆ†æ

```python
import json

# åŠ è¼‰çµæœ
with open('results/tool-calling-gpt-oss-20b-tau-0.0_range_0--1_user-gpt-4o-llm_01161234.json', 'r') as f:
    results = json.load(f)

# è¨ˆç®—æˆåŠŸç‡
total_tasks = len(results)
successful_tasks = sum(1 for r in results if r['reward'] >= 0.99)
success_rate = successful_tasks / total_tasks

print(f"ç¸½ä»»å‹™æ•¸: {total_tasks}")
print(f"æˆåŠŸä»»å‹™æ•¸: {successful_tasks}")
print(f"æˆåŠŸç‡: {success_rate:.2%}")

# æŸ¥çœ‹å¤±æ•—ä»»å‹™
failed_tasks = [r for r in results if r['reward'] < 0.99]
for task in failed_tasks[:5]:  # é¡¯ç¤ºå‰ 5 å€‹å¤±æ•—ä»»å‹™
    print(f"\nä»»å‹™ {task['task_id']}:")
    print(f"  çå‹µ: {task['reward']}")
    print(f"  ä¿¡æ¯: {task['info']}")
```

---

## 6ï¸âƒ£ æ•…éšœæ’é™¤

### å•é¡Œ 1: é€£æ¥éŒ¯èª¤ "Connection refused"

**åŸå› **: æ¨¡å‹æœå‹™æœªå•Ÿå‹•æˆ–ç«¯å£ä¸æ­£ç¢º

**è§£æ±ºæ–¹æ¡ˆ**:
1. ç¢ºèªæ¨¡å‹æœå‹™æ­£åœ¨é‹è¡Œ
2. æª¢æŸ¥ç«¯å£æ˜¯å¦æ­£ç¢ºï¼ˆvLLM: 8000, Ollama: 11434, fastapi: 8000ï¼‰
3. æ¸¬è©¦æœå‹™: `curl http://localhost:8000/v1/models`

### å•é¡Œ 2: æ¨¡å‹åŠ è¼‰å¤±æ•—

**åŸå› **: æ¨¡å‹è·¯å¾‘ä¸æ­£ç¢ºæˆ–æ¨¡å‹æ–‡ä»¶æå£

**è§£æ±ºæ–¹æ¡ˆ**:
1. ç¢ºèªæ¨¡å‹è·¯å¾‘å­˜åœ¨: `ls outputs/tau_bench_rl/merged_model`
2. æª¢æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨:
   - `config.json`
   - `tokenizer_config.json`
   - `model.safetensors` æˆ– `pytorch_model.bin`
3. å¦‚æœä½¿ç”¨ LoRA æ¨¡å‹ï¼Œç¢ºä¿åŸºç¤æ¨¡å‹è·¯å¾‘æ­£ç¢º

### å•é¡Œ 3: GPU å…§å­˜ä¸è¶³

**åŸå› **: GPT-OSS 20B æ˜¯å¤§å‹æ¨¡å‹ï¼Œéœ€è¦å¤§é‡ GPU å…§å­˜

**è§£æ±ºæ–¹æ¡ˆ**:
1. ä½¿ç”¨ 4-bit é‡åŒ–ï¼ˆè¨“ç·´æ™‚å·²ä½¿ç”¨ï¼‰:
   ```bash
   # vLLM ç¤ºä¾‹
   python -m vllm.entrypoints.openai.api_server \
       --model outputs/tau_bench_rl/merged_model \
       --quantization awq \  # æˆ– gptq, bitsandbytes
       --dtype float16
   ```

2. æ¸›å°‘ max_model_len:
   ```bash
   --max-model-len 1024  # å¾ 2048 æ¸›å°‘åˆ° 1024
   ```

3. ä½¿ç”¨å¤š GPU:
   ```bash
   --tensor-parallel-size 2  # ä½¿ç”¨ 2 å¼µ GPU
   ```

### å•é¡Œ 4: æ¨ç†é€Ÿåº¦æ…¢

**è§£æ±ºæ–¹æ¡ˆ**:
1. ä½¿ç”¨ vLLMï¼ˆæ¯” Hugging Face Transformers å¿« 10-20 å€ï¼‰
2. å•Ÿç”¨ PagedAttention:
   ```bash
   --enable-paged-attention
   ```
3. èª¿æ•´ batch size:
   ```bash
   --max-num-batched-tokens 8192
   ```

### å•é¡Œ 5: è¼¸å‡ºæ ¼å¼éŒ¯èª¤

**åŸå› **: æ¨¡å‹æœªæ­£ç¢ºå­¸ç¿’ JSON è¼¸å‡ºæ ¼å¼

**è§£æ±ºæ–¹æ¡ˆ**:
1. æª¢æŸ¥è¨“ç·´æ•¸æ“šæ ¼å¼
2. å¢åŠ è¨“ç·´ epoch
3. èª¿æ•´ç³»çµ±æç¤ºï¼ˆåœ¨ `tool_calling_agent.py` ä¸­ï¼‰
4. ä½¿ç”¨æ›´å¤§çš„ LoRA rankï¼ˆåœ¨è¨“ç·´æ™‚ï¼‰

---

## 7ï¸âƒ£ æ€§èƒ½å„ªåŒ–å»ºè­°

### è¨“ç·´å„ªåŒ–
- **å¢åŠ  LoRA rank**: å¾ 16 å¢åŠ åˆ° 32 æˆ– 64ï¼ˆéœ€è¦æ›´å¤šå…§å­˜ï¼‰
- **å¢åŠ è¨“ç·´æ•¸æ“š**: ä½¿ç”¨æ›´å¤š tau-bench ä»»å‹™
- **èª¿æ•´çå‹µå‡½æ•¸**: æ ¹æ“šæ¸¬è©¦çµæœèª¿æ•´æ¬Šé‡

### æ¨ç†å„ªåŒ–
- **ä½¿ç”¨ vLLM**: æœ€å¿«çš„æ¨ç†å¼•æ“
- **æ‰¹é‡æ¸¬è©¦**: å¢åŠ  `--max-concurrency` åƒæ•¸
- **ä½¿ç”¨ SSD**: å°‡æ¨¡å‹æ”¾åœ¨ SSD ä¸ŠåŠ é€ŸåŠ è¼‰

### æ¸¬è©¦ç­–ç•¥
1. **æ¼¸é€²å¼æ¸¬è©¦**: å…ˆæ¸¬è©¦å°‘é‡ä»»å‹™ï¼ˆ10-20å€‹ï¼‰é©—è­‰é…ç½®
2. **éŒ¯èª¤åˆ†æ**: åˆ†æå¤±æ•—æ¡ˆä¾‹ï¼Œèª¿æ•´æ¨¡å‹æˆ–æç¤º
3. **å¤šæ¬¡é‹è¡Œ**: ä½¿ç”¨ `--num-trials 3` é€²è¡Œå¤šæ¬¡æ¸¬è©¦ï¼Œè¨ˆç®— Pass@k

---

## 8ï¸âƒ£ å®Œæ•´æ¸¬è©¦æµç¨‹ç¤ºä¾‹

```bash
# Step 1: ç¢ºèªæ¨¡å‹å·²è¨“ç·´ä¸¦ä¿å­˜
ls outputs/gpt-oss-20b-tau-bench-rl/merged_model/

# Step 2: å•Ÿå‹• server æœå‹™ï¼ˆåœ¨ä¸€å€‹çµ‚ç«¯ï¼‰
python server.py

# Step 3: æ¸¬è©¦æœå‹™ï¼ˆåœ¨å¦ä¸€å€‹çµ‚ç«¯ï¼‰
curl http://localhost:8000/v1/models

# Step 4: ä¿®æ”¹ tau_bench/agents/tool_calling_agent.py
# ï¼ˆæŒ‰ç…§å‰é¢çš„èªªæ˜ä¿®æ”¹ base_urlï¼‰

# Step 5: é‹è¡Œå°è¦æ¨¡æ¸¬è©¦
python run.py \
    --model gpt-oss-20b-tau \
    --model-provider openai \
    --env retail \
    --agent-strategy tool-calling \
    --temperature 0.0 \
    --task-split test \
    --start-index 0 \
    --end-index 10 \
    --log-dir results/pilot

# Step 6: æª¢æŸ¥çµæœ
python -c "
import json
with open('results/pilot/tool-calling-gpt-oss-20b-tau-0.0_range_0-10_user-gpt-4o-llm_*.json', 'r') as f:
    results = json.load(f)
success_rate = sum(1 for r in results if r['reward'] >= 0.99) / len(results)
print(f'æˆåŠŸç‡: {success_rate:.2%}')
"

# Step 7: å¦‚æœæ•ˆæœè‰¯å¥½ï¼Œé‹è¡Œå®Œæ•´æ¸¬è©¦
python run.py \
    --model gpt-oss-20b-tau \
    --model-provider openai \
    --env retail \
    --agent-strategy tool-calling \
    --temperature 0.0 \
    --task-split test \
    --start-index 0 \
    --end-index -1 \
    --log-dir results/full-test \
    --num-trials 3
```

---

## 9ï¸âƒ£ é€²éš: ä½¿ç”¨ LiteLLM ä»£ç†

å¦‚æœæƒ³è¦æ›´éˆæ´»çš„é…ç½®ï¼Œå¯ä»¥ä½¿ç”¨ LiteLLM ä»£ç†æœå‹™å™¨ï¼š

```bash
# å®‰è£ LiteLLM
pip install litellm[proxy]

# å‰µå»ºé…ç½®æ–‡ä»¶ litellm_config.yaml
model_list:
  - model_name: gpt-oss-20b-tau
    litellm_params:
      model: openai/gpt-oss-20b-tau
      api_base: http://localhost:8000/v1
      api_key: EMPTY

# å•Ÿå‹•ä»£ç†
litellm --config litellm_config.yaml --port 4000

# åœ¨ tau-bench ä¸­ä½¿ç”¨
# base_url: http://localhost:4000
```

---

## ğŸ“š ç›¸é—œè³‡æº

- [vLLM æ–‡æª”](https://docs.vllm.ai/)
- [Ollama æ–‡æª”](https://ollama.com/docs)
- [Text Generation Inference](https://huggingface.co/docs/text-generation-inference)
- [Tau-Bench GitHub](https://github.com/sierra-research/tau-bench)
- [Unsloth æ–‡æª”](https://github.com/unslothai/unsloth)

---

## â“ å¸¸è¦‹å•é¡Œ

**Q: æˆ‘æ‡‰è©²ä½¿ç”¨ final_model é‚„æ˜¯ merged_modelï¼Ÿ**

A: ä½¿ç”¨ `merged_model`ï¼Œå®ƒæ˜¯å®Œæ•´çš„æ¨¡å‹ï¼Œä¸éœ€è¦é¡å¤–çš„åŸºç¤æ¨¡å‹ã€‚

**Q: æ¸¬è©¦æ™‚éœ€è¦ä½¿ç”¨ GPU å—ï¼Ÿ**

A: æ˜¯çš„ï¼ŒGPT-OSS 20B æ˜¯å¤§å‹æ¨¡å‹ï¼Œéœ€è¦è‡³å°‘ 1 å¼µ GPUï¼ˆå»ºè­° A100 æˆ– H100ï¼‰ã€‚

**Q: å¦‚ä½•æé«˜æ¸¬è©¦é€Ÿåº¦ï¼Ÿ**

A: ä½¿ç”¨ vLLM + å¢åŠ  `--max-concurrency` + ä½¿ç”¨å¤šå¼µ GPUã€‚

**Q: è¨“ç·´æ™‚ä½¿ç”¨çš„ chat template æ˜¯ä»€éº¼ï¼Ÿ**

A: notebook ä¸­ä½¿ç”¨ `chatml` æ ¼å¼ã€‚å¦‚æœæ¨¡å‹è¼¸å‡ºæ ¼å¼æœ‰å•é¡Œï¼Œæª¢æŸ¥ tokenizer çš„ chat template é…ç½®ã€‚

---

## âœ… å¿«é€Ÿæª¢æŸ¥æ¸…å–®

æ¸¬è©¦å‰ç¢ºèªï¼š
- [ ] æ¨¡å‹å·²è¨“ç·´ä¸¦ä¿å­˜åˆ° `outputs/tau_bench_rl/merged_model/`
- [ ] æ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼ˆconfig.json, tokenizer files, model weightsï¼‰
- [ ] vLLM/Ollama/TGI æœå‹™å·²å•Ÿå‹•ä¸¦å¯è¨ªå•
- [ ] `tool_calling_agent.py` å·²é…ç½®æ­£ç¢ºçš„ base_url
- [ ] GPU å…§å­˜è¶³å¤ ï¼ˆè‡³å°‘ 40GBï¼‰
- [ ] ç¶²çµ¡é€£æ¥æ­£å¸¸ï¼ˆå¦‚æœä½¿ç”¨é ç¨‹ user_modelï¼‰

æ¸¬è©¦ä¸­æª¢æŸ¥ï¼š
- [ ] æ¨¡å‹èƒ½æ­£ç¢ºåŠ è¼‰å’ŒéŸ¿æ‡‰
- [ ] è¼¸å‡ºæ ¼å¼ç‚ºæœ‰æ•ˆçš„ JSON
- [ ] Tool calls æ ¼å¼æ­£ç¢º
- [ ] æ¨ç†é€Ÿåº¦å¯æ¥å—ï¼ˆæ¯å€‹ä»»å‹™ < 30 ç§’ï¼‰

æ¸¬è©¦å¾Œåˆ†æï¼š
- [ ] æˆåŠŸç‡ç¬¦åˆé æœŸ
- [ ] å¤±æ•—æ¡ˆä¾‹æœ‰æ˜ç¢ºåŸå› 
- [ ] çµæœå·²ä¿å­˜ä¸¦å‚™ä»½

---
