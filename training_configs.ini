# Tau-Bench RL Training Configuration
# =====================================
# This file contains configuration presets for different hardware setups

[default]
model_name = unsloth/gpt-oss-20b
dataset_path = dataset.jsonl
envs_path = envs/retail
output_dir = outputs/tau_bench_rl
max_seq_length = 2048
lora_rank = 16
load_in_4bit = true
offload_embedding = true
num_epochs = 3
batch_size = 2
gradient_accumulation_steps = 8
learning_rate = 5e-5
warmup_steps = 100
logging_steps = 10
save_steps = 100

# =====================================
# A100 40GB Configuration (4-bit)
# =====================================
[a100_40gb]
model_name = unsloth/gpt-oss-20b
max_seq_length = 2048
lora_rank = 16
load_in_4bit = true
offload_embedding = true
batch_size = 2
gradient_accumulation_steps = 8
learning_rate = 5e-5
# Effective batch size: 2 * 8 = 16

# =====================================
# A100 80GB Configuration (4-bit)
# =====================================
[a100_80gb_4bit]
model_name = unsloth/gpt-oss-20b
max_seq_length = 2048
lora_rank = 32
load_in_4bit = true
offload_embedding = true
batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 5e-5
# Effective batch size: 4 * 4 = 16

# =====================================
# A100 80GB Configuration (16-bit)
# =====================================
[a100_80gb_16bit]
model_name = unsloth/gpt-oss-20b
max_seq_length = 2048
lora_rank = 16
load_in_4bit = false
offload_embedding = false
batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 5e-5
# Effective batch size: 4 * 4 = 16
# Note: 16-bit training is faster than 4-bit

# =====================================
# 4x A100 80GB Configuration
# =====================================
[multi_a100_80gb]
model_name = unsloth/gpt-oss-20b
max_seq_length = 2048
lora_rank = 32
load_in_4bit = false
offload_embedding = false
batch_size = 8
gradient_accumulation_steps = 2
learning_rate = 5e-5
# Effective batch size: 8 * 2 * 4 = 64

# =====================================
# H100 80GB Configuration
# =====================================
[h100_80gb]
model_name = unsloth/gpt-oss-20b-BF16
max_seq_length = 2048
lora_rank = 32
load_in_4bit = false
offload_embedding = false
batch_size = 16
gradient_accumulation_steps = 1
learning_rate = 5e-5
# Effective batch size: 16
# Note: H100 has higher throughput, can use larger batch

# =====================================
# 8x H100 80GB Configuration
# =====================================
[multi_h100_80gb]
model_name = unsloth/gpt-oss-20b-BF16
max_seq_length = 2048
lora_rank = 64
load_in_4bit = false
offload_embedding = false
batch_size = 16
gradient_accumulation_steps = 1
learning_rate = 5e-5
# Effective batch size: 16 * 8 = 128

# =====================================
# Development/Testing Configuration
# =====================================
[dev]
model_name = unsloth/gpt-oss-20b
max_seq_length = 1024
lora_rank = 8
load_in_4bit = true
offload_embedding = true
num_epochs = 1
batch_size = 1
gradient_accumulation_steps = 2
learning_rate = 5e-5
warmup_steps = 10
logging_steps = 1
save_steps = 50
# Minimal config for quick testing

# =====================================
# High Quality Configuration
# =====================================
[high_quality]
model_name = unsloth/gpt-oss-20b
max_seq_length = 3072
lora_rank = 64
load_in_4bit = false
offload_embedding = false
num_epochs = 10
batch_size = 8
gradient_accumulation_steps = 2
learning_rate = 3e-5
warmup_steps = 500
logging_steps = 5
save_steps = 50
# For best quality, requires significant compute

# =====================================
# Memory Constrained Configuration
# =====================================
[memory_constrained]
model_name = unsloth/gpt-oss-20b
max_seq_length = 1024
lora_rank = 8
load_in_4bit = true
offload_embedding = true
batch_size = 1
gradient_accumulation_steps = 16
learning_rate = 5e-5
# For very limited VRAM (e.g., consumer GPUs)

# =====================================
# Multi-Node Configuration (2 nodes, 8 GPUs each)
# =====================================
[multi_node]
model_name = unsloth/gpt-oss-20b-BF16
max_seq_length = 2048
lora_rank = 64
load_in_4bit = false
offload_embedding = false
batch_size = 16
gradient_accumulation_steps = 1
learning_rate = 5e-5
# Effective batch size: 16 * 8 * 2 = 256
# Requires proper distributed training setup
