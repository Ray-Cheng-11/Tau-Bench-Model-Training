"""
Compare tasks generated by Blueprint (direct) vs AgentFlow.

Usage:
    python scripts/compare_blueprint_agentflow.py --num-tasks 10

The script will:
- Optionally run `task_pipeline.py` to generate `generated_tasks/test_direct.json` and
  `generated_tasks/test_agentflow.json` (will invoke with --agentflow for agentflow)
- Load the generated JSON files and compute metrics:
  - format_valid (has q, agt, ogt)
  - validator.valid (via TaskValidator)
  - user_id consistency (simple heuristic)
  - avg actions per task, avg outputs per task
  - fraction referencing real data (based on TaskValidator validate report)
- Save a comparison report to `generated_tasks/comparison_report_{ts}.json`

Notes:
- Requires the repository environment to be runnable (dependencies installed).
- Generation requires configured envs and API keys (the same as running `task_pipeline.py`).
"""

import argparse
import json
import os
import subprocess
import sys
import time
from datetime import datetime, timezone
from collections import Counter

ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
GENERATED_DIR = os.path.join(ROOT, 'generated_tasks')

# Import project utilities
sys.path.insert(0, ROOT)
try:
    from data_reader import TauBenchDataReader
    from task_generator import TaskValidator
    from task_pipeline import UserIDValidator
except Exception as e:
    # We'll still allow the script to run in a degraded mode
    TauBenchDataReader = None
    TaskValidator = None
    UserIDValidator = None
    print("Warning: Could not import project classes; running in degraded mode:", e)


def run_generation(mode: str, num_tasks: int, out_path: str, agentflow_turns: int = 5):
    """Run task_pipeline.py to generate tasks.
    mode: 'direct' or 'agentflow'
    """
    # Force child python to run in UTF-8 mode to avoid Windows console encoding issues
    # (emoji printouts in task_pipeline can raise UnicodeEncodeError on cp950)
    cmd = [sys.executable, '-X', 'utf8', os.path.join(ROOT, 'task_pipeline.py'), '--num-tasks', str(num_tasks), '--output', out_path]
    if mode == 'agentflow':
        cmd += ['--agentflow', '--agentflow-turns', str(agentflow_turns)]
    print('Running generation:', ' '.join(cmd))
    # Ensure the child process uses UTF-8 for IO. Setting both PYTHONUTF8 and
    # PYTHONIOENCODING helps on Windows where the console encoding (cp950) may
    # not accept emoji characters.
    env = os.environ.copy()
    env['PYTHONUTF8'] = '1'
    env['PYTHONIOENCODING'] = 'utf-8'

    # Use explicit UTF-8 decoding and replace errors to avoid Windows cp950 decode issues
    proc = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='replace', env=env)
    print('Return code:', proc.returncode)
    if proc.stdout:
        print('stdout (truncated):', proc.stdout[:4000])
    if proc.stderr:
        print('stderr (truncated):', proc.stderr[:4000])
    return proc.returncode == 0


def load_tasks(path: str):
    if not os.path.exists(path):
        return []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # If pipeline saves a dict wrapper, try to extract list
            if isinstance(data, dict) and data.get('tasks'):
                return data['tasks']
            if isinstance(data, list):
                return data
            # Fallback: wrap single task
            return [data]
    except Exception as e:
        print('Failed to load', path, e)
        return []


def format_valid(task):
    if not isinstance(task, dict):
        return False
    return all(k in task for k in ('q', 'agt', 'ogt'))


def user_id_consistency(task):
    # If UserIDValidator is available, use it
    try:
        if UserIDValidator is not None:
            ok, issues = UserIDValidator.validate_user_id_consistency(task)
            return ok
    except Exception:
        pass
    # Fallback heuristic: collect explicit user_id in each action's arguments
    if not isinstance(task, dict):
        return False
    agt = task.get('agt', []) or []
    uids = [ (a.get('arguments') or {}).get('user_id') for a in agt if isinstance(a, dict) ]
    uids = [u for u in uids if u]
    if not uids:
        return False
    return len(set(uids)) <= 1


def calculate_real_data_reference_ratio(tasks, data_reader=None):
    """
    Calculate the ratio of actions referencing real data (user_id/order_id/product_id in envs data).
    Also validates that order_id belongs to the same user.
    
    Returns:
        Dict with detailed statistics about real data references
    """
    if not tasks or not data_reader:
        return {
            'total_actions': 0,
            'total_id_references': 0,
            'valid_references': 0,
            'invalid_references': 0,
            'reference_ratio': 0.0,
            'details': {}
        }
    
    # Load real data
    try:
        data = data_reader.read_data_files()
        users = data.get('users', {})
        orders = data.get('orders', {})
        products = data.get('products', {})
    except Exception as e:
        print(f"Error loading data: {e}")
        return {
            'error': str(e),
            'total_actions': 0,
            'total_id_references': 0,
            'valid_references': 0,
            'invalid_references': 0,
            'reference_ratio': 0.0
        }
    
    total_actions = 0
    total_user_ids = 0
    valid_user_ids = 0
    invalid_user_ids = []
    
    total_order_ids = 0
    valid_order_ids = 0
    invalid_order_ids = []
    
    total_product_ids = 0
    valid_product_ids = 0
    invalid_product_ids = []
    
    # Order-User association validation
    total_order_user_checks = 0
    valid_order_user_associations = 0
    invalid_order_user_associations = []
    
    # Analyze each task
    for task in tasks:
        if not isinstance(task, dict):
            continue
        
        # Track the canonical user_id for this task
        task_user_ids = []
        
        actions = task.get('agt', [])
        if not isinstance(actions, list):
            continue
        
        # First pass: collect all user_ids in this task
        for action in actions:
            if not isinstance(action, dict):
                continue
            args = action.get('arguments', {})
            if isinstance(args, dict) and 'user_id' in args:
                task_user_ids.append(args['user_id'])
        
        # Determine canonical user_id (most common)
        canonical_user_id = None
        if task_user_ids:
            from collections import Counter
            canonical_user_id = Counter(task_user_ids).most_common(1)[0][0]
        
        for action in actions:
            if not isinstance(action, dict):
                continue
            
            total_actions += 1
            args = action.get('arguments', {})
            if not isinstance(args, dict):
                continue
            
            action_name = action.get('name', 'unknown')
            action_user_id = args.get('user_id')
            
            # Check user_id
            if 'user_id' in args:
                user_id = args['user_id']
                total_user_ids += 1
                if user_id in users:
                    valid_user_ids += 1
                else:
                    invalid_user_ids.append({
                        'user_id': user_id,
                        'action': action_name
                    })
            
            # Check order_id
            if 'order_id' in args:
                order_id = str(args['order_id'])
                total_order_ids += 1
                
                # Normalize order_id (handle #W prefix)
                normalized_oid = order_id.strip().strip('"\'')
                # Remove existing # or #W prefix if present
                if normalized_oid.startswith('#W'):
                    normalized_oid = normalized_oid[2:]
                elif normalized_oid.startswith('#'):
                    normalized_oid = normalized_oid[1:]
                # Remove W prefix if present
                if normalized_oid.startswith('W'):
                    normalized_oid = normalized_oid[1:]
                # Add standard #W prefix
                normalized_oid = f"#W{normalized_oid}"
                
                if normalized_oid in orders:
                    valid_order_ids += 1
                    
                    # Additional check: does this order belong to the user?
                    if action_user_id and canonical_user_id:
                        total_order_user_checks += 1
                        order_data = orders[normalized_oid]
                        order_owner = order_data.get('user_id')
                        
                        if order_owner != canonical_user_id:
                            invalid_order_user_associations.append({
                                'order_id': order_id,
                                'normalized': normalized_oid,
                                'order_owner': order_owner,
                                'action_user_id': action_user_id,
                                'canonical_user_id': canonical_user_id,
                                'action': action_name
                            })
                        else:
                            valid_order_user_associations += 1
                else:
                    invalid_order_ids.append({
                        'order_id': order_id,
                        'normalized': normalized_oid,
                        'action': action_name
                    })
            
            # Check product_id
            if 'product_id' in args:
                product_id = args['product_id']
                total_product_ids += 1
                if product_id in products:
                    valid_product_ids += 1
                else:
                    invalid_product_ids.append({
                        'product_id': product_id,
                        'action': action_name
                    })
    
    # Calculate totals
    total_id_references = total_user_ids + total_order_ids + total_product_ids
    valid_references = valid_user_ids + valid_order_ids + valid_product_ids
    invalid_references = len(invalid_user_ids) + len(invalid_order_ids) + len(invalid_product_ids)
    
    reference_ratio = (valid_references / total_id_references * 100) if total_id_references > 0 else 0.0
    order_user_match_ratio = (valid_order_user_associations / total_order_user_checks * 100) if total_order_user_checks > 0 else 0.0
    
    return {
        'total_actions': total_actions,
        'total_id_references': total_id_references,
        'valid_references': valid_references,
        'invalid_references': invalid_references,
        'reference_ratio': reference_ratio,
        'order_user_checks': total_order_user_checks,
        'valid_order_user_associations': valid_order_user_associations,
        'invalid_order_user_associations': len(invalid_order_user_associations),
        'order_user_match_ratio': order_user_match_ratio,
        'details': {
            'user_id': {
                'total': total_user_ids,
                'valid': valid_user_ids,
                'invalid': len(invalid_user_ids),
                'valid_ratio': (valid_user_ids / total_user_ids * 100) if total_user_ids > 0 else 0.0,
                'invalid_samples': invalid_user_ids[:5]  # Show first 5 invalid
            },
            'order_id': {
                'total': total_order_ids,
                'valid': valid_order_ids,
                'invalid': len(invalid_order_ids),
                'valid_ratio': (valid_order_ids / total_order_ids * 100) if total_order_ids > 0 else 0.0,
                'invalid_samples': invalid_order_ids[:5]
            },
            'product_id': {
                'total': total_product_ids,
                'valid': valid_product_ids,
                'invalid': len(invalid_product_ids),
                'valid_ratio': (valid_product_ids / total_product_ids * 100) if total_product_ids > 0 else 0.0,
                'invalid_samples': invalid_product_ids[:5]
            },
            'order_user_association': {
                'total_checks': total_order_user_checks,
                'valid': valid_order_user_associations,
                'invalid': len(invalid_order_user_associations),
                'valid_ratio': order_user_match_ratio,
                'invalid_samples': invalid_order_user_associations[:5]
            }
        }
    }


def summarize_tasks(tasks, validator=None, data_reader=None):
    n = len(tasks)
    if n == 0:
        return {
            'count': 0
        }
    fmt_valid = sum(1 for t in tasks if format_valid(t))
    uid_consistent = sum(1 for t in tasks if user_id_consistency(t))
    actions_counts = [len(t.get('agt') or []) if isinstance(t, dict) else 0 for t in tasks]
    outputs_counts = [len(t.get('ogt') or []) if isinstance(t, dict) else 0 for t in tasks]

    validator_valid = 0
    validator_reports = []
    if validator is not None:
        for t in tasks:
            try:
                r = validator.validate(t)
                validator_reports.append({'valid': getattr(r, 'valid', False), 'missing': getattr(r, 'missing', [])})
                if getattr(r, 'valid', False):
                    validator_valid += 1
            except Exception as e:
                validator_reports.append({'valid': False, 'error': str(e)})
    else:
        validator_reports = None

    # Calculate real data reference ratio
    real_data_stats = calculate_real_data_reference_ratio(tasks, data_reader)

    return {
        'count': n,
        'format_valid_count': fmt_valid,
        'format_valid_fraction': fmt_valid / n,
        'user_id_consistent_count': uid_consistent,
        'user_id_consistent_fraction': uid_consistent / n,
        'avg_actions': sum(actions_counts) / n,
        'median_actions': sorted(actions_counts)[len(actions_counts)//2],
        'avg_outputs': sum(outputs_counts) / n,
        'validator_valid_count': validator_valid,
        'validator_reports_sample': optimizer_sample(validator_reports, 5) if validator_reports else None,
        'real_data_reference': real_data_stats
    }


def optimizer_sample(lst, k):
    if lst is None:
        return None
    return lst[:k]


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--num-tasks', type=int, default=10)
    parser.add_argument('--direct-out', type=str, default=os.path.join(GENERATED_DIR, 'test_direct.json'))
    parser.add_argument('--agent-out', type=str, default=os.path.join(GENERATED_DIR, 'test_agentflow.json'))
    parser.add_argument('--agentflow-turns', type=int, default=5)
    parser.add_argument('--force-generate', action='store_true', help='Force regeneration even if files exist')
    args = parser.parse_args()

    os.makedirs(GENERATED_DIR, exist_ok=True)

    # If requested or file missing, run generation
    if args.force_generate or not os.path.exists(args.direct_out):
        ok = run_generation('direct', args.num_tasks, args.direct_out)
        if not ok:
            print('Direct generation failed or exited non-zero; continuing to attempt load of existing files')

    if args.force_generate or not os.path.exists(args.agent_out):
        ok = run_generation('agentflow', args.num_tasks, args.agent_out, agentflow_turns=args.agentflow_turns)
        if not ok:
            print('AgentFlow generation failed or exited non-zero; continuing to attempt load of existing files')

    # Load tasks
    direct_tasks = load_tasks(args.direct_out)
    agent_tasks = load_tasks(args.agent_out)

    # Instantiate validator if possible
    validator = None
    data_reader = None
    if TaskValidator is not None and TauBenchDataReader is not None:
        try:
            data_reader = TauBenchDataReader('envs/retail')
            validator = TaskValidator(data_reader)
        except Exception as e:
            print('Could not instantiate TaskValidator:', e)

    print('\nLoaded: direct=%d, agentflow=%d' % (len(direct_tasks), len(agent_tasks)))

    direct_summary = summarize_tasks(direct_tasks, validator=validator, data_reader=data_reader)
    agent_summary = summarize_tasks(agent_tasks, validator=validator, data_reader=data_reader)

    report = {
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'num_tasks_requested': args.num_tasks,
        'direct_out': args.direct_out,
        'agent_out': args.agent_out,
        'direct_summary': direct_summary,
        'agent_summary': agent_summary
    }

    ts = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
    out_report = os.path.join(GENERATED_DIR, f'comparison_report_{ts}.json')
    with open(out_report, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print('\nComparison report saved to:', out_report)
    
    # Print formatted summary
    print('\n' + '='*80)
    print('REAL DATA REFERENCE ANALYSIS')
    print('='*80)
    
    for mode_name, summary_key in [('DIRECT', 'direct_summary'), ('AGENTFLOW', 'agent_summary')]:
        summary = report.get(summary_key, {})
        real_data = summary.get('real_data_reference', {})
        
        print(f'\n{mode_name} MODE:')
        print(f'  Total Tasks: {summary.get("count", 0)}')
        print(f'  Total Actions: {real_data.get("total_actions", 0)}')
        print(f'  Total ID References: {real_data.get("total_id_references", 0)}')
        print(f'  Valid References: {real_data.get("valid_references", 0)}')
        print(f'  Invalid References: {real_data.get("invalid_references", 0)}')
        print(f'  ‚úÖ Reference Ratio: {real_data.get("reference_ratio", 0):.2f}%')
        
        # Order-User association info
        order_user_checks = real_data.get('order_user_checks', 0)
        if order_user_checks > 0:
            print(f'\n  üîó Order-User Association:')
            print(f'     Total Checks: {order_user_checks}')
            print(f'     Valid: {real_data.get("valid_order_user_associations", 0)}')
            print(f'     Invalid: {real_data.get("invalid_order_user_associations", 0)}')
            print(f'     ‚úÖ Match Ratio: {real_data.get("order_user_match_ratio", 0):.2f}%')
        
        details = real_data.get('details', {})
        if details:
            print(f'\n  ID Type Breakdown:')
            for id_type in ['user_id', 'order_id', 'product_id']:
                id_stats = details.get(id_type, {})
                if id_stats.get('total', 0) > 0:
                    print(f'    {id_type}:')
                    print(f'      Total: {id_stats.get("total", 0)}')
                    print(f'      Valid: {id_stats.get("valid", 0)} ({id_stats.get("valid_ratio", 0):.1f}%)')
                    print(f'      Invalid: {id_stats.get("invalid", 0)}')
                    
                    invalid_samples = id_stats.get('invalid_samples', [])
                    if invalid_samples:
                        print(f'      Invalid Samples:')
                        for sample in invalid_samples[:3]:
                            print(f'        - {sample}')
            
            # Show order-user association mismatches
            assoc_stats = details.get('order_user_association', {})
            if assoc_stats and assoc_stats.get('invalid', 0) > 0:
                print(f'\n  ‚ö†Ô∏è  Order-User Mismatches:')
                invalid_assoc = assoc_stats.get('invalid_samples', [])
                for sample in invalid_assoc[:3]:
                    print(f'      ‚Ä¢ Order {sample.get("order_id")} belongs to {sample.get("order_owner")}')
                    print(f'        but task uses user_id={sample.get("canonical_user_id")}')
                    print(f'        in action={sample.get("action")}')
    
    print('\n' + '='*80)
    print('\nFull JSON report:')
    print(json.dumps(report, indent=2, ensure_ascii=False))

if __name__ == '__main__':
    main()
